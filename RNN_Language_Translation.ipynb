{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Language Translation\n",
    "In this project, I’ll be training a sequence to sequence model on a dataset of English and French sentences that can translate new sentences from English to French.\n",
    "## Get the Data\n",
    "The data will be small subsets of both languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(20000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 20 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'\n",
    "source_text = helper.load_data(source_path)\n",
    "target_text = helper.load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Use function view_sentence_range to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 227\n",
      "Number of sentences: 137861\n",
      "Average number of words in a sentence: 13.225277634719028\n",
      "\n",
      "English sentences 0 to 10:\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "the united states is usually chilly during july , and it is usually freezing in november .\n",
      "california is usually quiet during march , and it is usually hot in june .\n",
      "the united states is sometimes mild during june , and it is cold in september .\n",
      "your least liked fruit is the grape , but my least liked is the apple .\n",
      "his favorite fruit is the orange , but my favorite is the grape .\n",
      "paris is relaxing during december , but it is usually chilly in july .\n",
      "new jersey is busy during spring , and it is never hot in march .\n",
      "our least liked fruit is the lemon , but my least liked is the grape .\n",
      "the united states is sometimes busy during january , and it is sometimes warm in november .\n",
      "\n",
      "French sentences 0 to 10:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "son fruit préféré est l'orange , mais mon préféré est le raisin .\n",
      "paris est relaxant en décembre , mais il est généralement froid en juillet .\n",
      "new jersey est occupé au printemps , et il est jamais chaude en mars .\n",
      "notre fruit est moins aimé le citron , mais mon moins aimé est le raisin .\n",
      "les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in source_text.split()})))\n",
    "\n",
    "sentences = source_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "\n",
    "print()\n",
    "print('English sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n",
    "print()\n",
    "print('French sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocessing Function\n",
    "### Text to Word Ids\n",
    "In the function `text_to_ids()`, `source_text` and `target_text` are processed from words to ids.  Additionally, the `<EOS>` word id is added at the end of `target_text`.  This will help the neural network predict when the sentence should end.\n",
    "\n",
    "You can get the `<EOS>` word id by doing:\n",
    "```python\n",
    "target_vocab_to_int['<EOS>']\n",
    "```\n",
    "You can get other word ids using `source_vocab_to_int` and `target_vocab_to_int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert source and target text to proper word ids\n",
    "    :param source_text: String that contains all the source text.\n",
    "    :param target_text: String that contains all the target text.\n",
    "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: A tuple of lists (source_id_text, target_id_text)\n",
    "    \"\"\"\n",
    "    source_id_text = []\n",
    "    target_id_text = []\n",
    "    \n",
    "    for sentence in source_text.split(\"\\n\"):\n",
    "        source_addition = []\n",
    "        for word in sentence.split():\n",
    "            source_addition.append(source_vocab_to_int[word])\n",
    "        source_id_text.append(source_addition)\n",
    "    \n",
    "    for sentence in target_text.split(\"\\n\"):\n",
    "        target_addition = []\n",
    "        for word in sentence.split():\n",
    "            target_addition.append(target_vocab_to_int[word])\n",
    "        target_addition.append(target_vocab_to_int['<EOS>'])\n",
    "        target_id_text.append(target_addition)\n",
    "\n",
    "    return source_id_text, target_id_text\n",
    "\n",
    "tests.test_text_to_ids(text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Version of TensorFlow and Access to GPU\n",
    "Check to make sure we have the correct version of TensorFlow and access to a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==1.1.0 in /Users/kosty/Developer/software/miniconda3/envs/tf_1-1-0/lib/python3.5/site-packages\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /Users/kosty/Developer/software/miniconda3/envs/tf_1-1-0/lib/python3.5/site-packages (from tensorflow==1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/kosty/Developer/software/miniconda3/envs/tf_1-1-0/lib/python3.5/site-packages (from tensorflow==1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Users/kosty/Developer/software/miniconda3/envs/tf_1-1-0/lib/python3.5/site-packages (from tensorflow==1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/kosty/Developer/software/miniconda3/envs/tf_1-1-0/lib/python3.5/site-packages (from tensorflow==1.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /Users/kosty/Developer/software/miniconda3/envs/tf_1-1-0/lib/python3.5/site-packages (from tensorflow==1.1.0)\n",
      "Requirement already satisfied: setuptools in /Users/kosty/Developer/software/miniconda3/envs/tf_1-1-0/lib/python3.5/site-packages (from protobuf>=3.2.0->tensorflow==1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kosty/Developer/software/miniconda3/envs/tf_1-1-0/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "Built the components necessary to build a Sequence-to-Sequence model by implementing the following functions below:\n",
    "- `model_inputs`\n",
    "- `process_decoder_input`\n",
    "- `encoding_layer`\n",
    "- `decoding_layer_train`\n",
    "- `decoding_layer_infer`\n",
    "- `decoding_layer`\n",
    "- `seq2seq_model`\n",
    "\n",
    "### Input\n",
    "Implement the `model_inputs()` function to create TF Placeholders for the Neural Network. It should create the following placeholders:\n",
    "\n",
    "- Input text placeholder named \"input\" using the TF Placeholder name parameter with rank 2.\n",
    "- Targets placeholder with rank 2.\n",
    "- Learning rate placeholder with rank 0.\n",
    "- Keep probability placeholder named \"keep_prob\" using the TF Placeholder name parameter with rank 0.\n",
    "- Target sequence length placeholder named \"target_sequence_length\" with rank 1\n",
    "- Max target sequence length tensor named \"max_target_len\" getting its value from applying tf.reduce_max on the target_sequence_length placeholder. Rank 0.\n",
    "- Source sequence length placeholder named \"source_sequence_length\" with rank 1\n",
    "\n",
    "Return the placeholders in the following the tuple (input, targets, learning rate, keep probability, target sequence length, max target sequence length, source sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, learning rate, and lengths of source and target sequences.\n",
    "    :return: Tuple (input, targets, learning rate, keep probability, target sequence length,\n",
    "    max target sequence length, source sequence length)\n",
    "    \"\"\"\n",
    "    # Create tf.placeholders\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name=\"input\")\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name=\"target\")\n",
    "    learning_rate = tf.placeholder(tf.float32, None, name=\"learning_rate\")\n",
    "    keep_prob = tf.placeholder(tf.float32, None, name=\"keep_prob\")\n",
    "    target_seq = tf.placeholder(tf.int32, [None], name=\"target_sequence_length\")\n",
    "    max_target_seq = tf.placeholder(tf.int32, name=\"max_target_len\")\n",
    "    source_seq = tf.placeholder(tf.int32, [None], name=\"source_sequence_length\")\n",
    "    #Set value\n",
    "    max_target_seq = tf.reduce_max(target_seq)\n",
    "    \n",
    "    return inputs, targets, learning_rate, keep_prob, target_seq, max_target_seq, source_seq\n",
    "\n",
    "tests.test_model_inputs(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Decoder Input\n",
    "Implement `process_decoder_input` by removing the last word id from each batch in `target_data` and concat the GO ID to the begining of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    :param target_data: Target Placehoder\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param batch_size: Batch Size\n",
    "    :return: Preprocessed target data\n",
    "tf.strided_slice(\n",
    "    input_,\n",
    "    begin,\n",
    "    end,\n",
    "    strides=None,\n",
    "    begin_mask=0,\n",
    "    end_mask=0,\n",
    "    ellipsis_mask=0,\n",
    "    new_axis_mask=0,\n",
    "    shrink_axis_mask=0,\n",
    "    var=None,\n",
    "    name=None\n",
    " )\n",
    "tf.fill(\n",
    "    dims,\n",
    "    value,\n",
    "    name=None\n",
    " )\n",
    "tf.concat(\n",
    "    values,\n",
    "    axis,\n",
    "    name='concat'\n",
    " )\n",
    "    \"\"\"\n",
    "    removed_last = tf.strided_slice(target_data, [0,0], [batch_size, -1], [1,1])\n",
    "    add_go = tf.fill([batch_size, 1], target_vocab_to_int['<GO>'])\n",
    "    \n",
    "    processed_target_data = tf.concat([add_go, removed_last], 1)\n",
    "    \n",
    "    return processed_target_data\n",
    "\n",
    "tests.test_process_encoding_input(process_decoder_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Implement `encoding_layer()` to create a Encoder RNN layer:\n",
    " * Embedded the encoder input using [`tf.contrib.layers.embed_sequence`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence)\n",
    " * Constructed a [stacked](https://github.com/tensorflow/tensorflow/blob/6947f65a374ebf29e74bb71e36fd82760056d82c/tensorflow/docs_src/tutorials/recurrent.md#stacking-multiple-lstms) [`tf.contrib.rnn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell) wrapped in a [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper)\n",
    " * Passed cell and embedded input to [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from imp import reload\n",
    "reload(tests)\n",
    "\n",
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create encoding layer\n",
    "    :param rnn_inputs: Inputs for the RNN\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :param source_sequence_length: a list of the lengths of each sequence in the batch\n",
    "    :param source_vocab_size: vocabulary size of source data\n",
    "    :param encoding_embedding_size: embedding size of source data\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "\n",
    "embed_sequence(\n",
    "    ids,\n",
    "    vocab_size=None,\n",
    "    embed_dim=None,\n",
    "    unique=False,\n",
    "    initializer=None,\n",
    "    regularizer=None,\n",
    "    trainable=True,\n",
    "    scope=None,\n",
    "    reuse=None\n",
    " )\n",
    "    \"\"\"\n",
    "    \n",
    "    embedded_encoder = tf.contrib.layers.embed_sequence(rnn_inputs, source_vocab_size, encoding_embedding_size)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(\n",
    "                                            rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1)) \n",
    "                                                for _ in range(num_layers)])\n",
    "    rnn_output, rnn_state = tf.nn.dynamic_rnn(cell, embedded_encoder, \n",
    "                                              sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    return rnn_output, rnn_state\n",
    "\n",
    "tests.test_encoding_layer(encoding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Training\n",
    "Create a training decoding layer:\n",
    "* Created a [`tf.contrib.seq2seq.TrainingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/TrainingHelper) \n",
    "* Created a [`tf.contrib.seq2seq.BasicDecoder`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder)\n",
    "* Obtained the decoder outputs from [`tf.contrib.seq2seq.dynamic_decode`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for training\n",
    "    :param encoder_state: Encoder State\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param target_sequence_length: The lengths of each sequence in the target batch\n",
    "    :param max_summary_length: The length of the longest sequence in the batch\n",
    "    :param output_layer: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "\n",
    "tf.contrib.seq2seq.TrainingHelper\n",
    "__init__(\n",
    "    inputs,\n",
    "    sequence_length,\n",
    "    time_major=False,\n",
    "    name=None\n",
    " )\n",
    "tf.contrib.seq2seq.BasicDecoder\n",
    "__init__(\n",
    "    cell,\n",
    "    helper,\n",
    "    initial_state,\n",
    "    output_layer=None\n",
    " )\n",
    "dynamic_decode(\n",
    "    decoder,\n",
    "    output_time_major=False,\n",
    "    impute_finished=False,\n",
    "    maximum_iterations=None,\n",
    "    parallel_iterations=32,\n",
    "    swap_memory=False,\n",
    "    scope=None\n",
    " )\n",
    "    \"\"\"\n",
    "    \n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, target_sequence_length)\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, train_helper, encoder_state, output_layer)\n",
    "\n",
    "    decoder_output, _ = tf.contrib.seq2seq.dynamic_decode(train_decoder, impute_finished=True, \n",
    "                                                          maximum_iterations = max_summary_length)\n",
    "    logits = decoder_output.rnn_output\n",
    "    sample_id = decoder_output.sample_id\n",
    "    logits = tf.nn.dropout(logits, keep_prob)\n",
    "    \n",
    "    return tf.contrib.seq2seq.BasicDecoderOutput(logits, sample_id)\n",
    "\n",
    "tests.test_decoding_layer_train(decoding_layer_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Inference\n",
    "Create inference decoder:\n",
    "* Create a [`tf.contrib.seq2seq.GreedyEmbeddingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/GreedyEmbeddingHelper)\n",
    "* Create a [`tf.contrib.seq2seq.BasicDecoder`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder)\n",
    "* Obtain the decoder outputs from [`tf.contrib.seq2seq.dynamic_decode`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for inference\n",
    "    :param encoder_state: Encoder state\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param start_of_sequence_id: GO ID\n",
    "    :param end_of_sequence_id: EOS Id\n",
    "    :param max_target_sequence_length: Maximum length of target sequences\n",
    "    :param vocab_size: Size of decoder/target vocabulary\n",
    "    :param decoding_scope: TenorFlow Variable Scope for decoding\n",
    "    :param output_layer: Function to apply the output layer\n",
    "    :param batch_size: Batch size\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "\n",
    "tf.contrib.seq2seq.GreedyEmbeddingHelper    \n",
    "    __init__(\n",
    "    embedding,\n",
    "    start_tokens,\n",
    "    end_token\n",
    " )\n",
    " \n",
    "tile(\n",
    "    input,\n",
    "    multiples,\n",
    "    name=None\n",
    " )\n",
    "    \"\"\"\n",
    "    # Create start tokens for GreedyEmbeddingHelper function\n",
    "    start_tokens = tf.tile(\n",
    "        tf.constant([start_of_sequence_id], dtype=tf.int32), [batch_size], \n",
    "        name='Start-Tokens')\n",
    "\n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, start_tokens, end_of_sequence_id)\n",
    "\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, encoder_state, output_layer)\n",
    "        \n",
    "    decoder_output, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True,\n",
    "                                                          maximum_iterations = max_target_sequence_length)\n",
    "    logits = decoder_output.rnn_output\n",
    "    sample_id = decoder_output.sample_id\n",
    "    logits = tf.nn.dropout(logits, keep_prob)\n",
    "    \n",
    "    return tf.contrib.seq2seq.BasicDecoderOutput(logits, sample_id)\n",
    "\n",
    "tests.test_decoding_layer_infer(decoding_layer_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Decoding Layer\n",
    "Implement `decoding_layer()` to create a Decoder RNN layer.\n",
    "\n",
    "* Embed the target sequences\n",
    "* Construct the decoder LSTM cell (just like you constructed the encoder cell above)\n",
    "* Create an output layer to map the outputs of the decoder to the elements of our vocabulary\n",
    "* Use the `decoding_layer_train(encoder_state, dec_cell, dec_embed_input, target_sequence_length, max_target_sequence_length, output_layer, keep_prob)` function to get the training logits.\n",
    "* Use the `decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, max_target_sequence_length, vocab_size, output_layer, batch_size, keep_prob)` function to get the inference logits.\n",
    "\n",
    "Note: [tf.variable_scope](https://www.tensorflow.org/api_docs/python/tf/variable_scope) to share variables between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :param dec_input: Decoder input\n",
    "    :param encoder_state: Encoder state\n",
    "    :param target_sequence_length: The lengths of each sequence in the target batch\n",
    "    :param max_target_sequence_length: Maximum length of target sequences\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param target_vocab_size: Size of target vocabulary\n",
    "    :param batch_size: The size of the batch\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :param decoding_embedding_size: Decoding embedding size\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "\n",
    "    def create_cell(rnn_size):\n",
    "        lstm = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1))\n",
    "        dropout = tf.contrib.rnn.DropoutWrapper(lstm, keep_prob)\n",
    "        return dropout\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n",
    "\n",
    "    output_layer = Dense(target_vocab_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        train_logits = decoding_layer_train(encoder_state, cell, embed_input, target_sequence_length, \n",
    "                                            max_target_sequence_length, output_layer, keep_prob) \n",
    "    \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_logits = decoding_layer_infer(encoder_state, cell, embeddings, target_vocab_to_int['<GO>'], \n",
    "                                            target_vocab_to_int['<EOS>'], max_target_sequence_length, \n",
    "                                            target_vocab_size, output_layer, batch_size, keep_prob)\n",
    "    return train_logits, infer_logits\n",
    "\n",
    "tests.test_decoding_layer(decoding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "\n",
    "- Encode the input using your `encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob,  source_sequence_length, source_vocab_size, encoding_embedding_size)`.\n",
    "- Process target data using your `process_decoder_input(target_data, target_vocab_to_int, batch_size)` function.\n",
    "- Decode the encoded input using your `decoding_layer(dec_input, enc_state, target_sequence_length, max_target_sentence_length, rnn_size, num_layers, target_vocab_to_int, target_vocab_size, batch_size, keep_prob, dec_embedding_size)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  source_sequence_length, target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence part of the neural network\n",
    "    :param input_data: Input placeholder\n",
    "    :param target_data: Target placeholder\n",
    "    :param keep_prob: Dropout keep probability placeholder\n",
    "    :param batch_size: Batch Size\n",
    "    :param source_sequence_length: Sequence Lengths of source sequences in the batch\n",
    "    :param target_sequence_length: Sequence Lengths of target sequences in the batch\n",
    "    :param source_vocab_size: Source vocabulary size\n",
    "    :param target_vocab_size: Target vocabulary size\n",
    "    :param enc_embedding_size: Decoder embedding size\n",
    "    :param dec_embedding_size: Encoder embedding size\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "\n",
    "    _, enc_state = encoding_layer(input_data, rnn_size, num_layers, keep_prob, source_sequence_length,\n",
    "                               source_vocab_size, enc_embedding_size)\n",
    "    \n",
    "    \n",
    "    decoder_input = process_decoder_input(target_data, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    trn_decoder_output, inf_decoder_output = decoding_layer(decoder_input, enc_state, target_sequence_length, \n",
    "                                                                  max_target_sentence_length, rnn_size, num_layers,\n",
    "                                                                  target_vocab_to_int, target_vocab_size, batch_size,\n",
    "                                                                  keep_prob, dec_embedding_size)\n",
    "    \n",
    "    return trn_decoder_output, inf_decoder_output\n",
    "\n",
    "tests.test_seq2seq_model(seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `num_layers` to the number of layers.\n",
    "- Set `encoding_embedding_size` to the size of the embedding for the encoder.\n",
    "- Set `decoding_embedding_size` to the size of the embedding for the decoder.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `keep_probability` to the Dropout keep probability\n",
    "- Set `display_step` to state how many steps between each debug output statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 5\n",
    "# Batch Size\n",
    "batch_size = 512\n",
    "# RNN Size\n",
    "rnn_size = 512\n",
    "# Number of Layers\n",
    "num_layers = 4\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 128\n",
    "decoding_embedding_size = 128\n",
    "# Learning Rate\n",
    "learning_rate = .001\n",
    "# Dropout Keep Probability\n",
    "keep_probability = .77\n",
    "display_step = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob, target_sequence_length, max_target_sequence_length, source_sequence_length = model_inputs()\n",
    "\n",
    "    #sequence_length = tf.placeholder_with_default(max_target_sentence_length, None, name='sequence_length')\n",
    "    input_shape = tf.shape(input_data)\n",
    "\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   source_sequence_length,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "\n",
    "\n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch and pad the source and target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Train the neural network on the preprocessed data. If you have a hard time getting a good loss, check the forms to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    2/269 - Train Accuracy: 0.0989, Validation Accuracy: 0.0909, Loss: 5.6460\n",
      "Epoch   0 Batch    4/269 - Train Accuracy: 0.2793, Validation Accuracy: 0.3502, Loss: 4.3136\n",
      "Epoch   0 Batch    6/269 - Train Accuracy: 0.3378, Validation Accuracy: 0.3664, Loss: 3.9786\n",
      "Epoch   0 Batch    8/269 - Train Accuracy: 0.3515, Validation Accuracy: 0.4121, Loss: 3.9034\n",
      "Epoch   0 Batch   10/269 - Train Accuracy: 0.3547, Validation Accuracy: 0.4168, Loss: 3.7250\n",
      "Epoch   0 Batch   12/269 - Train Accuracy: 0.3819, Validation Accuracy: 0.4347, Loss: 3.6611\n",
      "Epoch   0 Batch   14/269 - Train Accuracy: 0.4083, Validation Accuracy: 0.4417, Loss: 3.5223\n",
      "Epoch   0 Batch   16/269 - Train Accuracy: 0.4086, Validation Accuracy: 0.4353, Loss: 3.4353\n",
      "Epoch   0 Batch   18/269 - Train Accuracy: 0.4022, Validation Accuracy: 0.4576, Loss: 3.4321\n",
      "Epoch   0 Batch   20/269 - Train Accuracy: 0.4204, Validation Accuracy: 0.4665, Loss: 3.3772\n",
      "Epoch   0 Batch   22/269 - Train Accuracy: 0.4568, Validation Accuracy: 0.4790, Loss: 3.2569\n",
      "Epoch   0 Batch   24/269 - Train Accuracy: 0.3922, Validation Accuracy: 0.4525, Loss: 3.3236\n",
      "Epoch   0 Batch   26/269 - Train Accuracy: 0.4636, Validation Accuracy: 0.4592, Loss: 3.0565\n",
      "Epoch   0 Batch   28/269 - Train Accuracy: 0.4088, Validation Accuracy: 0.4747, Loss: 3.3130\n",
      "Epoch   0 Batch   30/269 - Train Accuracy: 0.4651, Validation Accuracy: 0.4885, Loss: 3.0736\n",
      "Epoch   0 Batch   32/269 - Train Accuracy: 0.4628, Validation Accuracy: 0.4985, Loss: 3.0782\n",
      "Epoch   0 Batch   34/269 - Train Accuracy: 0.4813, Validation Accuracy: 0.4997, Loss: 2.9943\n",
      "Epoch   0 Batch   36/269 - Train Accuracy: 0.4837, Validation Accuracy: 0.5006, Loss: 2.9829\n",
      "Epoch   0 Batch   38/269 - Train Accuracy: 0.4825, Validation Accuracy: 0.5057, Loss: 2.9112\n",
      "Epoch   0 Batch   40/269 - Train Accuracy: 0.4646, Validation Accuracy: 0.5121, Loss: 2.9525\n",
      "Epoch   0 Batch   42/269 - Train Accuracy: 0.5114, Validation Accuracy: 0.5094, Loss: 2.8111\n",
      "Epoch   0 Batch   44/269 - Train Accuracy: 0.5011, Validation Accuracy: 0.5111, Loss: 2.8037\n",
      "Epoch   0 Batch   46/269 - Train Accuracy: 0.4640, Validation Accuracy: 0.5067, Loss: 2.9152\n",
      "Epoch   0 Batch   48/269 - Train Accuracy: 0.4981, Validation Accuracy: 0.5136, Loss: 2.7134\n",
      "Epoch   0 Batch   50/269 - Train Accuracy: 0.4710, Validation Accuracy: 0.5229, Loss: 2.8886\n",
      "Epoch   0 Batch   52/269 - Train Accuracy: 0.4830, Validation Accuracy: 0.4971, Loss: 2.7020\n",
      "Epoch   0 Batch   54/269 - Train Accuracy: 0.4521, Validation Accuracy: 0.4927, Loss: 3.3178\n",
      "Epoch   0 Batch   56/269 - Train Accuracy: 0.5013, Validation Accuracy: 0.5126, Loss: 3.1501\n",
      "Epoch   0 Batch   58/269 - Train Accuracy: 0.4565, Validation Accuracy: 0.4806, Loss: 3.0723\n",
      "Epoch   0 Batch   60/269 - Train Accuracy: 0.4631, Validation Accuracy: 0.4624, Loss: 2.7130\n",
      "Epoch   0 Batch   62/269 - Train Accuracy: 0.4902, Validation Accuracy: 0.4973, Loss: 2.7914\n",
      "Epoch   0 Batch   64/269 - Train Accuracy: 0.4677, Validation Accuracy: 0.4968, Loss: 2.8309\n",
      "Epoch   0 Batch   66/269 - Train Accuracy: 0.4945, Validation Accuracy: 0.4972, Loss: 2.7025\n",
      "Epoch   0 Batch   68/269 - Train Accuracy: 0.4784, Validation Accuracy: 0.5107, Loss: 2.7411\n",
      "Epoch   0 Batch   70/269 - Train Accuracy: 0.5017, Validation Accuracy: 0.5080, Loss: 2.6824\n",
      "Epoch   0 Batch   72/269 - Train Accuracy: 0.5242, Validation Accuracy: 0.5264, Loss: 2.6643\n",
      "Epoch   0 Batch   74/269 - Train Accuracy: 0.4910, Validation Accuracy: 0.5278, Loss: 2.7391\n",
      "Epoch   0 Batch   76/269 - Train Accuracy: 0.4938, Validation Accuracy: 0.5291, Loss: 2.6786\n",
      "Epoch   0 Batch   78/269 - Train Accuracy: 0.5131, Validation Accuracy: 0.5286, Loss: 2.6466\n",
      "Epoch   0 Batch   80/269 - Train Accuracy: 0.5191, Validation Accuracy: 0.5313, Loss: 2.6408\n",
      "Epoch   0 Batch   82/269 - Train Accuracy: 0.5273, Validation Accuracy: 0.5345, Loss: 2.5631\n",
      "Epoch   0 Batch   84/269 - Train Accuracy: 0.5236, Validation Accuracy: 0.5318, Loss: 2.5907\n",
      "Epoch   0 Batch   86/269 - Train Accuracy: 0.5121, Validation Accuracy: 0.5368, Loss: 2.5468\n",
      "Epoch   0 Batch   88/269 - Train Accuracy: 0.5110, Validation Accuracy: 0.5218, Loss: 2.4948\n",
      "Epoch   0 Batch   90/269 - Train Accuracy: 0.3750, Validation Accuracy: 0.4535, Loss: 2.8434\n",
      "Epoch   0 Batch   92/269 - Train Accuracy: 0.4137, Validation Accuracy: 0.4351, Loss: 2.6818\n",
      "Epoch   0 Batch   94/269 - Train Accuracy: 0.4568, Validation Accuracy: 0.4869, Loss: 2.6428\n",
      "Epoch   0 Batch   96/269 - Train Accuracy: 0.4984, Validation Accuracy: 0.5179, Loss: 2.7191\n",
      "Epoch   0 Batch   98/269 - Train Accuracy: 0.5196, Validation Accuracy: 0.5228, Loss: 2.7024\n",
      "Epoch   0 Batch  100/269 - Train Accuracy: 0.5087, Validation Accuracy: 0.5105, Loss: 2.6773\n",
      "Epoch   0 Batch  102/269 - Train Accuracy: 0.4900, Validation Accuracy: 0.5048, Loss: 2.6215\n",
      "Epoch   0 Batch  104/269 - Train Accuracy: 0.4891, Validation Accuracy: 0.5165, Loss: 2.5932\n",
      "Epoch   0 Batch  106/269 - Train Accuracy: 0.5089, Validation Accuracy: 0.5328, Loss: 2.6239\n",
      "Epoch   0 Batch  108/269 - Train Accuracy: 0.5315, Validation Accuracy: 0.5440, Loss: 2.5888\n",
      "Epoch   0 Batch  110/269 - Train Accuracy: 0.5237, Validation Accuracy: 0.5440, Loss: 2.5559\n",
      "Epoch   0 Batch  112/269 - Train Accuracy: 0.5328, Validation Accuracy: 0.5457, Loss: 2.5397\n",
      "Epoch   0 Batch  114/269 - Train Accuracy: 0.5238, Validation Accuracy: 0.5405, Loss: 2.5388\n",
      "Epoch   0 Batch  116/269 - Train Accuracy: 0.5310, Validation Accuracy: 0.5407, Loss: 2.4619\n",
      "Epoch   0 Batch  118/269 - Train Accuracy: 0.5323, Validation Accuracy: 0.5450, Loss: 2.4455\n",
      "Epoch   0 Batch  120/269 - Train Accuracy: 0.5037, Validation Accuracy: 0.5411, Loss: 2.4663\n",
      "Epoch   0 Batch  122/269 - Train Accuracy: 0.5429, Validation Accuracy: 0.5522, Loss: 2.4171\n",
      "Epoch   0 Batch  124/269 - Train Accuracy: 0.5318, Validation Accuracy: 0.5573, Loss: 2.4287\n",
      "Epoch   0 Batch  126/269 - Train Accuracy: 0.5360, Validation Accuracy: 0.5513, Loss: 2.4192\n",
      "Epoch   0 Batch  128/269 - Train Accuracy: 0.5426, Validation Accuracy: 0.5502, Loss: 2.3941\n",
      "Epoch   0 Batch  130/269 - Train Accuracy: 0.4795, Validation Accuracy: 0.5267, Loss: 2.4510\n",
      "Epoch   0 Batch  132/269 - Train Accuracy: 0.5338, Validation Accuracy: 0.5508, Loss: 2.4012\n",
      "Epoch   0 Batch  134/269 - Train Accuracy: 0.5235, Validation Accuracy: 0.5667, Loss: 2.4313\n",
      "Epoch   0 Batch  136/269 - Train Accuracy: 0.5168, Validation Accuracy: 0.5598, Loss: 2.4289\n",
      "Epoch   0 Batch  138/269 - Train Accuracy: 0.5229, Validation Accuracy: 0.5641, Loss: 2.4022\n",
      "Epoch   0 Batch  140/269 - Train Accuracy: 0.5427, Validation Accuracy: 0.5550, Loss: 2.3300\n",
      "Epoch   0 Batch  142/269 - Train Accuracy: 0.5539, Validation Accuracy: 0.5554, Loss: 2.2831\n",
      "Epoch   0 Batch  144/269 - Train Accuracy: 0.5569, Validation Accuracy: 0.5650, Loss: 2.2172\n",
      "Epoch   0 Batch  146/269 - Train Accuracy: 0.5479, Validation Accuracy: 0.5642, Loss: 2.2740\n",
      "Epoch   0 Batch  148/269 - Train Accuracy: 0.5490, Validation Accuracy: 0.5678, Loss: 2.2726\n",
      "Epoch   0 Batch  150/269 - Train Accuracy: 0.5470, Validation Accuracy: 0.5573, Loss: 2.2535\n",
      "Epoch   0 Batch  152/269 - Train Accuracy: 0.5164, Validation Accuracy: 0.5318, Loss: 2.3539\n",
      "Epoch   0 Batch  154/269 - Train Accuracy: 0.4929, Validation Accuracy: 0.5288, Loss: 2.4075\n",
      "Epoch   0 Batch  156/269 - Train Accuracy: 0.4950, Validation Accuracy: 0.5304, Loss: 2.3084\n",
      "Epoch   0 Batch  158/269 - Train Accuracy: 0.5399, Validation Accuracy: 0.5475, Loss: 2.2888\n",
      "Epoch   0 Batch  160/269 - Train Accuracy: 0.5583, Validation Accuracy: 0.5622, Loss: 2.2623\n",
      "Epoch   0 Batch  162/269 - Train Accuracy: 0.5262, Validation Accuracy: 0.5459, Loss: 2.2304\n",
      "Epoch   0 Batch  164/269 - Train Accuracy: 0.5620, Validation Accuracy: 0.5720, Loss: 2.2399\n",
      "Epoch   0 Batch  166/269 - Train Accuracy: 0.5845, Validation Accuracy: 0.5682, Loss: 2.1949\n",
      "Epoch   0 Batch  168/269 - Train Accuracy: 0.5339, Validation Accuracy: 0.5544, Loss: 2.2661\n",
      "Epoch   0 Batch  170/269 - Train Accuracy: 0.5670, Validation Accuracy: 0.5758, Loss: 2.1634\n",
      "Epoch   0 Batch  172/269 - Train Accuracy: 0.5497, Validation Accuracy: 0.5700, Loss: 2.2073\n",
      "Epoch   0 Batch  174/269 - Train Accuracy: 0.5319, Validation Accuracy: 0.5646, Loss: 2.1732\n",
      "Epoch   0 Batch  176/269 - Train Accuracy: 0.5486, Validation Accuracy: 0.5779, Loss: 2.2279\n",
      "Epoch   0 Batch  178/269 - Train Accuracy: 0.5526, Validation Accuracy: 0.5709, Loss: 2.1957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  180/269 - Train Accuracy: 0.5637, Validation Accuracy: 0.5709, Loss: 2.1489\n",
      "Epoch   0 Batch  182/269 - Train Accuracy: 0.5605, Validation Accuracy: 0.5743, Loss: 2.1555\n",
      "Epoch   0 Batch  184/269 - Train Accuracy: 0.5312, Validation Accuracy: 0.5698, Loss: 2.1824\n",
      "Epoch   0 Batch  186/269 - Train Accuracy: 0.5344, Validation Accuracy: 0.5749, Loss: 2.1990\n",
      "Epoch   0 Batch  188/269 - Train Accuracy: 0.5709, Validation Accuracy: 0.5765, Loss: 2.0794\n",
      "Epoch   0 Batch  190/269 - Train Accuracy: 0.5591, Validation Accuracy: 0.5702, Loss: 2.0736\n",
      "Epoch   0 Batch  192/269 - Train Accuracy: 0.5611, Validation Accuracy: 0.5627, Loss: 2.0794\n",
      "Epoch   0 Batch  194/269 - Train Accuracy: 0.5626, Validation Accuracy: 0.5649, Loss: 2.0791\n",
      "Epoch   0 Batch  196/269 - Train Accuracy: 0.5537, Validation Accuracy: 0.5789, Loss: 2.1272\n",
      "Epoch   0 Batch  198/269 - Train Accuracy: 0.4923, Validation Accuracy: 0.5342, Loss: 2.2986\n",
      "Epoch   0 Batch  200/269 - Train Accuracy: 0.5213, Validation Accuracy: 0.5524, Loss: 2.1970\n",
      "Epoch   0 Batch  202/269 - Train Accuracy: 0.5512, Validation Accuracy: 0.5595, Loss: 2.1272\n",
      "Epoch   0 Batch  204/269 - Train Accuracy: 0.5219, Validation Accuracy: 0.5563, Loss: 2.1241\n",
      "Epoch   0 Batch  206/269 - Train Accuracy: 0.5229, Validation Accuracy: 0.5605, Loss: 2.1490\n",
      "Epoch   0 Batch  208/269 - Train Accuracy: 0.5268, Validation Accuracy: 0.5600, Loss: 2.1517\n",
      "Epoch   0 Batch  210/269 - Train Accuracy: 0.5817, Validation Accuracy: 0.5792, Loss: 2.0724\n",
      "Epoch   0 Batch  212/269 - Train Accuracy: 0.5694, Validation Accuracy: 0.5661, Loss: 2.0742\n",
      "Epoch   0 Batch  214/269 - Train Accuracy: 0.5618, Validation Accuracy: 0.5603, Loss: 2.0511\n",
      "Epoch   0 Batch  216/269 - Train Accuracy: 0.5427, Validation Accuracy: 0.5768, Loss: 2.1281\n",
      "Epoch   0 Batch  218/269 - Train Accuracy: 0.5521, Validation Accuracy: 0.5698, Loss: 2.0388\n",
      "Epoch   0 Batch  220/269 - Train Accuracy: 0.5742, Validation Accuracy: 0.5680, Loss: 1.9898\n",
      "Epoch   0 Batch  222/269 - Train Accuracy: 0.5709, Validation Accuracy: 0.5736, Loss: 1.9743\n",
      "Epoch   0 Batch  224/269 - Train Accuracy: 0.5573, Validation Accuracy: 0.5648, Loss: 2.0632\n",
      "Epoch   0 Batch  226/269 - Train Accuracy: 0.5771, Validation Accuracy: 0.5753, Loss: 2.0053\n",
      "Epoch   0 Batch  228/269 - Train Accuracy: 0.5515, Validation Accuracy: 0.5746, Loss: 2.0147\n",
      "Epoch   0 Batch  230/269 - Train Accuracy: 0.5571, Validation Accuracy: 0.5644, Loss: 2.0217\n",
      "Epoch   0 Batch  232/269 - Train Accuracy: 0.5406, Validation Accuracy: 0.5675, Loss: 2.0235\n",
      "Epoch   0 Batch  234/269 - Train Accuracy: 0.5571, Validation Accuracy: 0.5550, Loss: 2.0129\n",
      "Epoch   0 Batch  236/269 - Train Accuracy: 0.5656, Validation Accuracy: 0.5757, Loss: 2.0068\n",
      "Epoch   0 Batch  238/269 - Train Accuracy: 0.6017, Validation Accuracy: 0.5938, Loss: 2.0211\n",
      "Epoch   0 Batch  240/269 - Train Accuracy: 0.5971, Validation Accuracy: 0.5711, Loss: 1.8965\n",
      "Epoch   0 Batch  242/269 - Train Accuracy: 0.5384, Validation Accuracy: 0.5525, Loss: 1.9989\n",
      "Epoch   0 Batch  244/269 - Train Accuracy: 0.5756, Validation Accuracy: 0.5724, Loss: 1.9759\n",
      "Epoch   0 Batch  246/269 - Train Accuracy: 0.5551, Validation Accuracy: 0.5804, Loss: 1.9827\n",
      "Epoch   0 Batch  248/269 - Train Accuracy: 0.5878, Validation Accuracy: 0.5882, Loss: 1.9916\n",
      "Epoch   0 Batch  250/269 - Train Accuracy: 0.5420, Validation Accuracy: 0.5718, Loss: 1.9935\n",
      "Epoch   0 Batch  252/269 - Train Accuracy: 0.6015, Validation Accuracy: 0.5899, Loss: 1.9754\n",
      "Epoch   0 Batch  254/269 - Train Accuracy: 0.5684, Validation Accuracy: 0.5740, Loss: 1.9352\n",
      "Epoch   0 Batch  256/269 - Train Accuracy: 0.5706, Validation Accuracy: 0.5807, Loss: 1.9750\n",
      "Epoch   0 Batch  258/269 - Train Accuracy: 0.5531, Validation Accuracy: 0.5515, Loss: 1.9989\n",
      "Epoch   0 Batch  260/269 - Train Accuracy: 0.5558, Validation Accuracy: 0.5786, Loss: 2.0351\n",
      "Epoch   0 Batch  262/269 - Train Accuracy: 0.5991, Validation Accuracy: 0.5965, Loss: 1.9650\n",
      "Epoch   0 Batch  264/269 - Train Accuracy: 0.5332, Validation Accuracy: 0.5662, Loss: 2.0415\n",
      "Epoch   0 Batch  266/269 - Train Accuracy: 0.5728, Validation Accuracy: 0.5644, Loss: 1.9192\n",
      "Epoch   1 Batch    2/269 - Train Accuracy: 0.5701, Validation Accuracy: 0.5927, Loss: 1.9639\n",
      "Epoch   1 Batch    4/269 - Train Accuracy: 0.5615, Validation Accuracy: 0.5963, Loss: 2.0212\n",
      "Epoch   1 Batch    6/269 - Train Accuracy: 0.5992, Validation Accuracy: 0.5987, Loss: 1.8705\n",
      "Epoch   1 Batch    8/269 - Train Accuracy: 0.5693, Validation Accuracy: 0.5961, Loss: 1.9869\n",
      "Epoch   1 Batch   10/269 - Train Accuracy: 0.5585, Validation Accuracy: 0.5811, Loss: 1.9196\n",
      "Epoch   1 Batch   12/269 - Train Accuracy: 0.5709, Validation Accuracy: 0.5911, Loss: 1.9422\n",
      "Epoch   1 Batch   14/269 - Train Accuracy: 0.5964, Validation Accuracy: 0.5933, Loss: 1.9201\n",
      "Epoch   1 Batch   16/269 - Train Accuracy: 0.6106, Validation Accuracy: 0.5969, Loss: 1.8980\n",
      "Epoch   1 Batch   18/269 - Train Accuracy: 0.5669, Validation Accuracy: 0.5915, Loss: 1.9585\n",
      "Epoch   1 Batch   20/269 - Train Accuracy: 0.5854, Validation Accuracy: 0.6017, Loss: 1.9743\n",
      "Epoch   1 Batch   22/269 - Train Accuracy: 0.6030, Validation Accuracy: 0.6072, Loss: 1.8800\n",
      "Epoch   1 Batch   24/269 - Train Accuracy: 0.5622, Validation Accuracy: 0.5917, Loss: 1.9696\n",
      "Epoch   1 Batch   26/269 - Train Accuracy: 0.6262, Validation Accuracy: 0.5945, Loss: 1.8743\n",
      "Epoch   1 Batch   28/269 - Train Accuracy: 0.5321, Validation Accuracy: 0.5819, Loss: 1.9802\n",
      "Epoch   1 Batch   30/269 - Train Accuracy: 0.5957, Validation Accuracy: 0.5993, Loss: 1.8923\n",
      "Epoch   1 Batch   32/269 - Train Accuracy: 0.5723, Validation Accuracy: 0.5874, Loss: 1.8855\n",
      "Epoch   1 Batch   34/269 - Train Accuracy: 0.6079, Validation Accuracy: 0.6071, Loss: 1.9081\n",
      "Epoch   1 Batch   36/269 - Train Accuracy: 0.6018, Validation Accuracy: 0.5985, Loss: 1.8499\n",
      "Epoch   1 Batch   38/269 - Train Accuracy: 0.6041, Validation Accuracy: 0.6064, Loss: 1.8903\n",
      "Epoch   1 Batch   40/269 - Train Accuracy: 0.5929, Validation Accuracy: 0.6108, Loss: 1.9082\n",
      "Epoch   1 Batch   42/269 - Train Accuracy: 0.6289, Validation Accuracy: 0.6025, Loss: 1.8472\n",
      "Epoch   1 Batch   44/269 - Train Accuracy: 0.6090, Validation Accuracy: 0.6125, Loss: 1.8919\n",
      "Epoch   1 Batch   46/269 - Train Accuracy: 0.5950, Validation Accuracy: 0.6113, Loss: 1.9051\n",
      "Epoch   1 Batch   48/269 - Train Accuracy: 0.6096, Validation Accuracy: 0.6021, Loss: 1.8513\n",
      "Epoch   1 Batch   50/269 - Train Accuracy: 0.5778, Validation Accuracy: 0.6064, Loss: 1.8846\n",
      "Epoch   1 Batch   52/269 - Train Accuracy: 0.6077, Validation Accuracy: 0.6036, Loss: 1.8564\n",
      "Epoch   1 Batch   54/269 - Train Accuracy: 0.5992, Validation Accuracy: 0.5999, Loss: 1.8834\n",
      "Epoch   1 Batch   56/269 - Train Accuracy: 0.6203, Validation Accuracy: 0.6038, Loss: 1.8463\n",
      "Epoch   1 Batch   58/269 - Train Accuracy: 0.6064, Validation Accuracy: 0.6056, Loss: 1.8524\n",
      "Epoch   1 Batch   60/269 - Train Accuracy: 0.6141, Validation Accuracy: 0.6096, Loss: 1.8087\n",
      "Epoch   1 Batch   62/269 - Train Accuracy: 0.6286, Validation Accuracy: 0.6119, Loss: 1.8261\n",
      "Epoch   1 Batch   64/269 - Train Accuracy: 0.6003, Validation Accuracy: 0.6092, Loss: 1.8243\n",
      "Epoch   1 Batch   66/269 - Train Accuracy: 0.6115, Validation Accuracy: 0.6028, Loss: 1.7927\n",
      "Epoch   1 Batch   68/269 - Train Accuracy: 0.6009, Validation Accuracy: 0.6067, Loss: 1.8199\n",
      "Epoch   1 Batch   70/269 - Train Accuracy: 0.5962, Validation Accuracy: 0.5976, Loss: 1.8015\n",
      "Epoch   1 Batch   72/269 - Train Accuracy: 0.6126, Validation Accuracy: 0.6104, Loss: 1.7828\n",
      "Epoch   1 Batch   74/269 - Train Accuracy: 0.6074, Validation Accuracy: 0.6176, Loss: 1.8186\n",
      "Epoch   1 Batch   76/269 - Train Accuracy: 0.6100, Validation Accuracy: 0.6119, Loss: 1.8054\n",
      "Epoch   1 Batch   78/269 - Train Accuracy: 0.6254, Validation Accuracy: 0.6159, Loss: 1.8079\n",
      "Epoch   1 Batch   80/269 - Train Accuracy: 0.6209, Validation Accuracy: 0.6106, Loss: 1.8034\n",
      "Epoch   1 Batch   82/269 - Train Accuracy: 0.6070, Validation Accuracy: 0.6132, Loss: 1.7661\n",
      "Epoch   1 Batch   84/269 - Train Accuracy: 0.6239, Validation Accuracy: 0.6174, Loss: 1.7589\n",
      "Epoch   1 Batch   86/269 - Train Accuracy: 0.5943, Validation Accuracy: 0.6108, Loss: 1.8017\n",
      "Epoch   1 Batch   88/269 - Train Accuracy: 0.6225, Validation Accuracy: 0.6196, Loss: 1.7897\n",
      "Epoch   1 Batch   90/269 - Train Accuracy: 0.5831, Validation Accuracy: 0.6216, Loss: 1.7926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch   92/269 - Train Accuracy: 0.6044, Validation Accuracy: 0.6182, Loss: 1.7832\n",
      "Epoch   1 Batch   94/269 - Train Accuracy: 0.6252, Validation Accuracy: 0.6214, Loss: 1.7750\n",
      "Epoch   1 Batch   96/269 - Train Accuracy: 0.6297, Validation Accuracy: 0.6214, Loss: 1.7737\n",
      "Epoch   1 Batch   98/269 - Train Accuracy: 0.6287, Validation Accuracy: 0.6325, Loss: 1.7477\n",
      "Epoch   1 Batch  100/269 - Train Accuracy: 0.6418, Validation Accuracy: 0.6218, Loss: 1.7800\n",
      "Epoch   1 Batch  102/269 - Train Accuracy: 0.6137, Validation Accuracy: 0.6165, Loss: 1.7848\n",
      "Epoch   1 Batch  104/269 - Train Accuracy: 0.6065, Validation Accuracy: 0.6132, Loss: 1.7352\n",
      "Epoch   1 Batch  106/269 - Train Accuracy: 0.6238, Validation Accuracy: 0.6222, Loss: 1.7574\n",
      "Epoch   1 Batch  108/269 - Train Accuracy: 0.6297, Validation Accuracy: 0.6269, Loss: 1.7526\n",
      "Epoch   1 Batch  110/269 - Train Accuracy: 0.6154, Validation Accuracy: 0.6345, Loss: 1.7256\n",
      "Epoch   1 Batch  112/269 - Train Accuracy: 0.6298, Validation Accuracy: 0.6169, Loss: 1.7530\n",
      "Epoch   1 Batch  114/269 - Train Accuracy: 0.6017, Validation Accuracy: 0.6086, Loss: 1.7258\n",
      "Epoch   1 Batch  116/269 - Train Accuracy: 0.6145, Validation Accuracy: 0.6232, Loss: 1.7397\n",
      "Epoch   1 Batch  118/269 - Train Accuracy: 0.6354, Validation Accuracy: 0.6309, Loss: 1.7100\n",
      "Epoch   1 Batch  120/269 - Train Accuracy: 0.6161, Validation Accuracy: 0.6229, Loss: 1.7684\n",
      "Epoch   1 Batch  122/269 - Train Accuracy: 0.6272, Validation Accuracy: 0.6295, Loss: 1.7051\n",
      "Epoch   1 Batch  124/269 - Train Accuracy: 0.6175, Validation Accuracy: 0.6175, Loss: 1.7005\n",
      "Epoch   1 Batch  126/269 - Train Accuracy: 0.6258, Validation Accuracy: 0.6287, Loss: 1.7714\n",
      "Epoch   1 Batch  128/269 - Train Accuracy: 0.6331, Validation Accuracy: 0.6260, Loss: 1.7262\n",
      "Epoch   1 Batch  130/269 - Train Accuracy: 0.6058, Validation Accuracy: 0.6221, Loss: 1.7669\n",
      "Epoch   1 Batch  132/269 - Train Accuracy: 0.6241, Validation Accuracy: 0.6265, Loss: 1.7017\n",
      "Epoch   1 Batch  134/269 - Train Accuracy: 0.5964, Validation Accuracy: 0.6108, Loss: 1.7835\n",
      "Epoch   1 Batch  136/269 - Train Accuracy: 0.5943, Validation Accuracy: 0.6262, Loss: 1.7619\n",
      "Epoch   1 Batch  138/269 - Train Accuracy: 0.6057, Validation Accuracy: 0.6219, Loss: 1.7216\n",
      "Epoch   1 Batch  140/269 - Train Accuracy: 0.6221, Validation Accuracy: 0.6239, Loss: 1.7161\n",
      "Epoch   1 Batch  142/269 - Train Accuracy: 0.6392, Validation Accuracy: 0.6322, Loss: 1.7341\n",
      "Epoch   1 Batch  144/269 - Train Accuracy: 0.6456, Validation Accuracy: 0.6341, Loss: 1.7052\n",
      "Epoch   1 Batch  146/269 - Train Accuracy: 0.6130, Validation Accuracy: 0.6179, Loss: 1.6757\n",
      "Epoch   1 Batch  148/269 - Train Accuracy: 0.6189, Validation Accuracy: 0.6130, Loss: 1.7213\n",
      "Epoch   1 Batch  150/269 - Train Accuracy: 0.6251, Validation Accuracy: 0.6254, Loss: 1.7208\n",
      "Epoch   1 Batch  152/269 - Train Accuracy: 0.6266, Validation Accuracy: 0.6308, Loss: 1.6945\n",
      "Epoch   1 Batch  154/269 - Train Accuracy: 0.6167, Validation Accuracy: 0.6295, Loss: 1.6764\n",
      "Epoch   1 Batch  156/269 - Train Accuracy: 0.6221, Validation Accuracy: 0.6393, Loss: 1.7139\n",
      "Epoch   1 Batch  158/269 - Train Accuracy: 0.6336, Validation Accuracy: 0.6353, Loss: 1.6848\n",
      "Epoch   1 Batch  160/269 - Train Accuracy: 0.6418, Validation Accuracy: 0.6328, Loss: 1.6595\n",
      "Epoch   1 Batch  162/269 - Train Accuracy: 0.6209, Validation Accuracy: 0.6332, Loss: 1.6440\n",
      "Epoch   1 Batch  164/269 - Train Accuracy: 0.6387, Validation Accuracy: 0.6343, Loss: 1.6843\n",
      "Epoch   1 Batch  166/269 - Train Accuracy: 0.6552, Validation Accuracy: 0.6367, Loss: 1.6413\n",
      "Epoch   1 Batch  168/269 - Train Accuracy: 0.6355, Validation Accuracy: 0.6414, Loss: 1.6753\n",
      "Epoch   1 Batch  170/269 - Train Accuracy: 0.6441, Validation Accuracy: 0.6437, Loss: 1.6758\n",
      "Epoch   1 Batch  172/269 - Train Accuracy: 0.6357, Validation Accuracy: 0.6372, Loss: 1.6763\n",
      "Epoch   1 Batch  174/269 - Train Accuracy: 0.6412, Validation Accuracy: 0.6420, Loss: 1.6490\n",
      "Epoch   1 Batch  176/269 - Train Accuracy: 0.6339, Validation Accuracy: 0.6507, Loss: 1.7119\n",
      "Epoch   1 Batch  178/269 - Train Accuracy: 0.6410, Validation Accuracy: 0.6456, Loss: 1.6607\n",
      "Epoch   1 Batch  180/269 - Train Accuracy: 0.6493, Validation Accuracy: 0.6433, Loss: 1.6382\n",
      "Epoch   1 Batch  182/269 - Train Accuracy: 0.6486, Validation Accuracy: 0.6440, Loss: 1.6541\n",
      "Epoch   1 Batch  184/269 - Train Accuracy: 0.6403, Validation Accuracy: 0.6533, Loss: 1.6629\n",
      "Epoch   1 Batch  186/269 - Train Accuracy: 0.6212, Validation Accuracy: 0.6450, Loss: 1.6589\n",
      "Epoch   1 Batch  188/269 - Train Accuracy: 0.6589, Validation Accuracy: 0.6535, Loss: 1.6416\n",
      "Epoch   1 Batch  190/269 - Train Accuracy: 0.6416, Validation Accuracy: 0.6517, Loss: 1.6315\n",
      "Epoch   1 Batch  192/269 - Train Accuracy: 0.6507, Validation Accuracy: 0.6469, Loss: 1.6559\n",
      "Epoch   1 Batch  194/269 - Train Accuracy: 0.6600, Validation Accuracy: 0.6544, Loss: 1.6262\n",
      "Epoch   1 Batch  196/269 - Train Accuracy: 0.6334, Validation Accuracy: 0.6523, Loss: 1.6767\n",
      "Epoch   1 Batch  198/269 - Train Accuracy: 0.6282, Validation Accuracy: 0.6474, Loss: 1.6563\n",
      "Epoch   1 Batch  200/269 - Train Accuracy: 0.6427, Validation Accuracy: 0.6519, Loss: 1.6737\n",
      "Epoch   1 Batch  202/269 - Train Accuracy: 0.6527, Validation Accuracy: 0.6515, Loss: 1.6147\n",
      "Epoch   1 Batch  204/269 - Train Accuracy: 0.6336, Validation Accuracy: 0.6561, Loss: 1.6514\n",
      "Epoch   1 Batch  206/269 - Train Accuracy: 0.6235, Validation Accuracy: 0.6437, Loss: 1.6765\n",
      "Epoch   1 Batch  208/269 - Train Accuracy: 0.6399, Validation Accuracy: 0.6459, Loss: 1.6701\n",
      "Epoch   1 Batch  210/269 - Train Accuracy: 0.6589, Validation Accuracy: 0.6485, Loss: 1.6111\n",
      "Epoch   1 Batch  212/269 - Train Accuracy: 0.6443, Validation Accuracy: 0.6404, Loss: 1.5979\n",
      "Epoch   1 Batch  214/269 - Train Accuracy: 0.6651, Validation Accuracy: 0.6582, Loss: 1.6154\n",
      "Epoch   1 Batch  216/269 - Train Accuracy: 0.6371, Validation Accuracy: 0.6620, Loss: 1.7009\n",
      "Epoch   1 Batch  218/269 - Train Accuracy: 0.6449, Validation Accuracy: 0.6621, Loss: 1.6294\n",
      "Epoch   1 Batch  220/269 - Train Accuracy: 0.6561, Validation Accuracy: 0.6569, Loss: 1.5694\n",
      "Epoch   1 Batch  222/269 - Train Accuracy: 0.6713, Validation Accuracy: 0.6670, Loss: 1.6050\n",
      "Epoch   1 Batch  224/269 - Train Accuracy: 0.6566, Validation Accuracy: 0.6639, Loss: 1.6593\n",
      "Epoch   1 Batch  226/269 - Train Accuracy: 0.6546, Validation Accuracy: 0.6531, Loss: 1.6104\n",
      "Epoch   1 Batch  228/269 - Train Accuracy: 0.6423, Validation Accuracy: 0.6462, Loss: 1.6658\n",
      "Epoch   1 Batch  230/269 - Train Accuracy: 0.6526, Validation Accuracy: 0.6405, Loss: 1.6026\n",
      "Epoch   1 Batch  232/269 - Train Accuracy: 0.6307, Validation Accuracy: 0.6554, Loss: 1.6368\n",
      "Epoch   1 Batch  234/269 - Train Accuracy: 0.6596, Validation Accuracy: 0.6586, Loss: 1.6214\n",
      "Epoch   1 Batch  236/269 - Train Accuracy: 0.6402, Validation Accuracy: 0.6577, Loss: 1.5872\n",
      "Epoch   1 Batch  238/269 - Train Accuracy: 0.6598, Validation Accuracy: 0.6629, Loss: 1.5863\n",
      "Epoch   1 Batch  240/269 - Train Accuracy: 0.6650, Validation Accuracy: 0.6348, Loss: 1.5703\n",
      "Epoch   1 Batch  242/269 - Train Accuracy: 0.6512, Validation Accuracy: 0.6574, Loss: 1.6166\n",
      "Epoch   1 Batch  244/269 - Train Accuracy: 0.6572, Validation Accuracy: 0.6659, Loss: 1.5895\n",
      "Epoch   1 Batch  246/269 - Train Accuracy: 0.6492, Validation Accuracy: 0.6689, Loss: 1.5507\n",
      "Epoch   1 Batch  248/269 - Train Accuracy: 0.6541, Validation Accuracy: 0.6681, Loss: 1.6259\n",
      "Epoch   1 Batch  250/269 - Train Accuracy: 0.6379, Validation Accuracy: 0.6545, Loss: 1.6167\n",
      "Epoch   1 Batch  252/269 - Train Accuracy: 0.6548, Validation Accuracy: 0.6484, Loss: 1.5825\n",
      "Epoch   1 Batch  254/269 - Train Accuracy: 0.6670, Validation Accuracy: 0.6717, Loss: 1.5984\n",
      "Epoch   1 Batch  256/269 - Train Accuracy: 0.6446, Validation Accuracy: 0.6675, Loss: 1.6046\n",
      "Epoch   1 Batch  258/269 - Train Accuracy: 0.6629, Validation Accuracy: 0.6650, Loss: 1.6008\n",
      "Epoch   1 Batch  260/269 - Train Accuracy: 0.6313, Validation Accuracy: 0.6687, Loss: 1.6012\n",
      "Epoch   1 Batch  262/269 - Train Accuracy: 0.6590, Validation Accuracy: 0.6618, Loss: 1.5574\n",
      "Epoch   1 Batch  264/269 - Train Accuracy: 0.6341, Validation Accuracy: 0.6666, Loss: 1.6475\n",
      "Epoch   1 Batch  266/269 - Train Accuracy: 0.6702, Validation Accuracy: 0.6655, Loss: 1.5758\n",
      "Epoch   2 Batch    2/269 - Train Accuracy: 0.6576, Validation Accuracy: 0.6746, Loss: 1.5750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2 Batch    4/269 - Train Accuracy: 0.6521, Validation Accuracy: 0.6710, Loss: 1.5646\n",
      "Epoch   2 Batch    6/269 - Train Accuracy: 0.6666, Validation Accuracy: 0.6648, Loss: 1.5504\n",
      "Epoch   2 Batch    8/269 - Train Accuracy: 0.6540, Validation Accuracy: 0.6797, Loss: 1.5878\n",
      "Epoch   2 Batch   10/269 - Train Accuracy: 0.6612, Validation Accuracy: 0.6784, Loss: 1.5535\n",
      "Epoch   2 Batch   12/269 - Train Accuracy: 0.6492, Validation Accuracy: 0.6827, Loss: 1.5771\n",
      "Epoch   2 Batch   14/269 - Train Accuracy: 0.6568, Validation Accuracy: 0.6773, Loss: 1.5338\n",
      "Epoch   2 Batch   16/269 - Train Accuracy: 0.6860, Validation Accuracy: 0.6782, Loss: 1.5378\n",
      "Epoch   2 Batch   18/269 - Train Accuracy: 0.6437, Validation Accuracy: 0.6508, Loss: 1.5677\n",
      "Epoch   2 Batch   20/269 - Train Accuracy: 0.6618, Validation Accuracy: 0.6822, Loss: 1.5401\n",
      "Epoch   2 Batch   22/269 - Train Accuracy: 0.6760, Validation Accuracy: 0.6760, Loss: 1.5661\n",
      "Epoch   2 Batch   24/269 - Train Accuracy: 0.6498, Validation Accuracy: 0.6766, Loss: 1.5718\n",
      "Epoch   2 Batch   26/269 - Train Accuracy: 0.6980, Validation Accuracy: 0.6730, Loss: 1.5034\n",
      "Epoch   2 Batch   28/269 - Train Accuracy: 0.6374, Validation Accuracy: 0.6683, Loss: 1.6197\n",
      "Epoch   2 Batch   30/269 - Train Accuracy: 0.6878, Validation Accuracy: 0.6810, Loss: 1.5171\n",
      "Epoch   2 Batch   32/269 - Train Accuracy: 0.6681, Validation Accuracy: 0.6802, Loss: 1.5346\n",
      "Epoch   2 Batch   34/269 - Train Accuracy: 0.6685, Validation Accuracy: 0.6805, Loss: 1.5138\n",
      "Epoch   2 Batch   36/269 - Train Accuracy: 0.6847, Validation Accuracy: 0.6837, Loss: 1.5215\n",
      "Epoch   2 Batch   38/269 - Train Accuracy: 0.6807, Validation Accuracy: 0.6804, Loss: 1.5309\n",
      "Epoch   2 Batch   40/269 - Train Accuracy: 0.6790, Validation Accuracy: 0.6914, Loss: 1.5553\n",
      "Epoch   2 Batch   42/269 - Train Accuracy: 0.6996, Validation Accuracy: 0.6797, Loss: 1.4890\n",
      "Epoch   2 Batch   44/269 - Train Accuracy: 0.6855, Validation Accuracy: 0.6803, Loss: 1.5106\n",
      "Epoch   2 Batch   46/269 - Train Accuracy: 0.6865, Validation Accuracy: 0.6940, Loss: 1.5197\n",
      "Epoch   2 Batch   48/269 - Train Accuracy: 0.6964, Validation Accuracy: 0.6913, Loss: 1.5220\n",
      "Epoch   2 Batch   50/269 - Train Accuracy: 0.6839, Validation Accuracy: 0.6913, Loss: 1.5270\n",
      "Epoch   2 Batch   52/269 - Train Accuracy: 0.6969, Validation Accuracy: 0.6902, Loss: 1.4937\n",
      "Epoch   2 Batch   54/269 - Train Accuracy: 0.7065, Validation Accuracy: 0.6986, Loss: 1.5193\n",
      "Epoch   2 Batch   56/269 - Train Accuracy: 0.6931, Validation Accuracy: 0.6932, Loss: 1.4880\n",
      "Epoch   2 Batch   58/269 - Train Accuracy: 0.6992, Validation Accuracy: 0.6975, Loss: 1.4873\n",
      "Epoch   2 Batch   60/269 - Train Accuracy: 0.7140, Validation Accuracy: 0.6966, Loss: 1.4684\n",
      "Epoch   2 Batch   62/269 - Train Accuracy: 0.7290, Validation Accuracy: 0.7016, Loss: 1.4779\n",
      "Epoch   2 Batch   64/269 - Train Accuracy: 0.6949, Validation Accuracy: 0.6957, Loss: 1.4912\n",
      "Epoch   2 Batch   66/269 - Train Accuracy: 0.7093, Validation Accuracy: 0.7093, Loss: 1.4977\n",
      "Epoch   2 Batch   68/269 - Train Accuracy: 0.7040, Validation Accuracy: 0.6956, Loss: 1.5152\n",
      "Epoch   2 Batch   70/269 - Train Accuracy: 0.7181, Validation Accuracy: 0.6974, Loss: 1.5051\n",
      "Epoch   2 Batch   72/269 - Train Accuracy: 0.7027, Validation Accuracy: 0.6981, Loss: 1.4697\n",
      "Epoch   2 Batch   74/269 - Train Accuracy: 0.6984, Validation Accuracy: 0.6921, Loss: 1.4950\n",
      "Epoch   2 Batch   76/269 - Train Accuracy: 0.7137, Validation Accuracy: 0.7011, Loss: 1.4559\n",
      "Epoch   2 Batch   78/269 - Train Accuracy: 0.7253, Validation Accuracy: 0.6982, Loss: 1.4509\n",
      "Epoch   2 Batch   80/269 - Train Accuracy: 0.7254, Validation Accuracy: 0.7068, Loss: 1.4566\n",
      "Epoch   2 Batch   82/269 - Train Accuracy: 0.7223, Validation Accuracy: 0.7138, Loss: 1.4936\n",
      "Epoch   2 Batch   84/269 - Train Accuracy: 0.7238, Validation Accuracy: 0.7028, Loss: 1.4662\n",
      "Epoch   2 Batch   86/269 - Train Accuracy: 0.6971, Validation Accuracy: 0.6966, Loss: 1.5003\n",
      "Epoch   2 Batch   88/269 - Train Accuracy: 0.7179, Validation Accuracy: 0.7086, Loss: 1.4978\n",
      "Epoch   2 Batch   90/269 - Train Accuracy: 0.6793, Validation Accuracy: 0.7125, Loss: 1.5005\n",
      "Epoch   2 Batch   92/269 - Train Accuracy: 0.7033, Validation Accuracy: 0.7019, Loss: 1.4737\n",
      "Epoch   2 Batch   94/269 - Train Accuracy: 0.7076, Validation Accuracy: 0.7118, Loss: 1.4811\n",
      "Epoch   2 Batch   96/269 - Train Accuracy: 0.7065, Validation Accuracy: 0.7104, Loss: 1.4903\n",
      "Epoch   2 Batch   98/269 - Train Accuracy: 0.7106, Validation Accuracy: 0.7166, Loss: 1.4823\n",
      "Epoch   2 Batch  100/269 - Train Accuracy: 0.7454, Validation Accuracy: 0.7281, Loss: 1.4507\n",
      "Epoch   2 Batch  102/269 - Train Accuracy: 0.7238, Validation Accuracy: 0.7095, Loss: 1.4415\n",
      "Epoch   2 Batch  104/269 - Train Accuracy: 0.7212, Validation Accuracy: 0.7084, Loss: 1.4714\n",
      "Epoch   2 Batch  106/269 - Train Accuracy: 0.7219, Validation Accuracy: 0.7134, Loss: 1.4685\n",
      "Epoch   2 Batch  108/269 - Train Accuracy: 0.7041, Validation Accuracy: 0.7086, Loss: 1.4356\n",
      "Epoch   2 Batch  110/269 - Train Accuracy: 0.7304, Validation Accuracy: 0.7208, Loss: 1.4357\n",
      "Epoch   2 Batch  112/269 - Train Accuracy: 0.7139, Validation Accuracy: 0.7077, Loss: 1.4928\n",
      "Epoch   2 Batch  114/269 - Train Accuracy: 0.7042, Validation Accuracy: 0.7202, Loss: 1.4374\n",
      "Epoch   2 Batch  116/269 - Train Accuracy: 0.7521, Validation Accuracy: 0.7289, Loss: 1.4492\n",
      "Epoch   2 Batch  118/269 - Train Accuracy: 0.7474, Validation Accuracy: 0.7333, Loss: 1.4271\n",
      "Epoch   2 Batch  120/269 - Train Accuracy: 0.7446, Validation Accuracy: 0.7241, Loss: 1.4906\n",
      "Epoch   2 Batch  122/269 - Train Accuracy: 0.7371, Validation Accuracy: 0.7332, Loss: 1.4632\n",
      "Epoch   2 Batch  124/269 - Train Accuracy: 0.7372, Validation Accuracy: 0.7363, Loss: 1.4760\n",
      "Epoch   2 Batch  126/269 - Train Accuracy: 0.7365, Validation Accuracy: 0.7219, Loss: 1.4213\n",
      "Epoch   2 Batch  128/269 - Train Accuracy: 0.7400, Validation Accuracy: 0.7415, Loss: 1.4101\n",
      "Epoch   2 Batch  130/269 - Train Accuracy: 0.7252, Validation Accuracy: 0.7211, Loss: 1.4661\n",
      "Epoch   2 Batch  132/269 - Train Accuracy: 0.7210, Validation Accuracy: 0.7302, Loss: 1.4219\n",
      "Epoch   2 Batch  134/269 - Train Accuracy: 0.7170, Validation Accuracy: 0.7349, Loss: 1.4395\n",
      "Epoch   2 Batch  136/269 - Train Accuracy: 0.7310, Validation Accuracy: 0.7449, Loss: 1.4965\n",
      "Epoch   2 Batch  138/269 - Train Accuracy: 0.7323, Validation Accuracy: 0.7427, Loss: 1.4343\n",
      "Epoch   2 Batch  140/269 - Train Accuracy: 0.7480, Validation Accuracy: 0.7359, Loss: 1.4614\n",
      "Epoch   2 Batch  142/269 - Train Accuracy: 0.7364, Validation Accuracy: 0.7250, Loss: 1.4642\n",
      "Epoch   2 Batch  144/269 - Train Accuracy: 0.7543, Validation Accuracy: 0.7374, Loss: 1.4389\n",
      "Epoch   2 Batch  146/269 - Train Accuracy: 0.7497, Validation Accuracy: 0.7496, Loss: 1.4167\n",
      "Epoch   2 Batch  148/269 - Train Accuracy: 0.7449, Validation Accuracy: 0.7477, Loss: 1.4136\n",
      "Epoch   2 Batch  150/269 - Train Accuracy: 0.7488, Validation Accuracy: 0.7532, Loss: 1.4221\n",
      "Epoch   2 Batch  152/269 - Train Accuracy: 0.7409, Validation Accuracy: 0.7456, Loss: 1.3966\n",
      "Epoch   2 Batch  154/269 - Train Accuracy: 0.7579, Validation Accuracy: 0.7524, Loss: 1.4235\n",
      "Epoch   2 Batch  156/269 - Train Accuracy: 0.7491, Validation Accuracy: 0.7452, Loss: 1.4540\n",
      "Epoch   2 Batch  158/269 - Train Accuracy: 0.7512, Validation Accuracy: 0.7474, Loss: 1.4224\n",
      "Epoch   2 Batch  160/269 - Train Accuracy: 0.7676, Validation Accuracy: 0.7563, Loss: 1.4532\n",
      "Epoch   2 Batch  162/269 - Train Accuracy: 0.7722, Validation Accuracy: 0.7642, Loss: 1.4238\n",
      "Epoch   2 Batch  164/269 - Train Accuracy: 0.7675, Validation Accuracy: 0.7585, Loss: 1.3806\n",
      "Epoch   2 Batch  166/269 - Train Accuracy: 0.7787, Validation Accuracy: 0.7572, Loss: 1.3757\n",
      "Epoch   2 Batch  168/269 - Train Accuracy: 0.7625, Validation Accuracy: 0.7538, Loss: 1.4016\n",
      "Epoch   2 Batch  170/269 - Train Accuracy: 0.7533, Validation Accuracy: 0.7511, Loss: 1.3990\n",
      "Epoch   2 Batch  172/269 - Train Accuracy: 0.7642, Validation Accuracy: 0.7634, Loss: 1.3948\n",
      "Epoch   2 Batch  174/269 - Train Accuracy: 0.7520, Validation Accuracy: 0.7512, Loss: 1.3934\n",
      "Epoch   2 Batch  176/269 - Train Accuracy: 0.7603, Validation Accuracy: 0.7457, Loss: 1.4047\n",
      "Epoch   2 Batch  178/269 - Train Accuracy: 0.7681, Validation Accuracy: 0.7579, Loss: 1.3683\n",
      "Epoch   2 Batch  180/269 - Train Accuracy: 0.7921, Validation Accuracy: 0.7710, Loss: 1.4231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2 Batch  182/269 - Train Accuracy: 0.7780, Validation Accuracy: 0.7581, Loss: 1.4151\n",
      "Epoch   2 Batch  184/269 - Train Accuracy: 0.7695, Validation Accuracy: 0.7662, Loss: 1.4111\n",
      "Epoch   2 Batch  186/269 - Train Accuracy: 0.7582, Validation Accuracy: 0.7687, Loss: 1.3952\n",
      "Epoch   2 Batch  188/269 - Train Accuracy: 0.7744, Validation Accuracy: 0.7499, Loss: 1.3916\n",
      "Epoch   2 Batch  190/269 - Train Accuracy: 0.7872, Validation Accuracy: 0.7675, Loss: 1.3586\n",
      "Epoch   2 Batch  192/269 - Train Accuracy: 0.7873, Validation Accuracy: 0.7745, Loss: 1.3863\n",
      "Epoch   2 Batch  194/269 - Train Accuracy: 0.7737, Validation Accuracy: 0.7671, Loss: 1.3765\n",
      "Epoch   2 Batch  196/269 - Train Accuracy: 0.7671, Validation Accuracy: 0.7701, Loss: 1.3711\n",
      "Epoch   2 Batch  198/269 - Train Accuracy: 0.7616, Validation Accuracy: 0.7763, Loss: 1.3844\n",
      "Epoch   2 Batch  200/269 - Train Accuracy: 0.7952, Validation Accuracy: 0.7733, Loss: 1.3913\n",
      "Epoch   2 Batch  202/269 - Train Accuracy: 0.7751, Validation Accuracy: 0.7671, Loss: 1.3592\n",
      "Epoch   2 Batch  204/269 - Train Accuracy: 0.7589, Validation Accuracy: 0.7750, Loss: 1.4173\n",
      "Epoch   2 Batch  206/269 - Train Accuracy: 0.7898, Validation Accuracy: 0.7708, Loss: 1.3999\n",
      "Epoch   2 Batch  208/269 - Train Accuracy: 0.7852, Validation Accuracy: 0.7677, Loss: 1.3491\n",
      "Epoch   2 Batch  210/269 - Train Accuracy: 0.7892, Validation Accuracy: 0.7817, Loss: 1.3432\n",
      "Epoch   2 Batch  212/269 - Train Accuracy: 0.7913, Validation Accuracy: 0.7799, Loss: 1.3627\n",
      "Epoch   2 Batch  214/269 - Train Accuracy: 0.7958, Validation Accuracy: 0.7885, Loss: 1.3591\n",
      "Epoch   2 Batch  216/269 - Train Accuracy: 0.7761, Validation Accuracy: 0.7797, Loss: 1.3764\n",
      "Epoch   2 Batch  218/269 - Train Accuracy: 0.7854, Validation Accuracy: 0.7683, Loss: 1.3772\n",
      "Epoch   2 Batch  220/269 - Train Accuracy: 0.8031, Validation Accuracy: 0.7812, Loss: 1.3354\n",
      "Epoch   2 Batch  222/269 - Train Accuracy: 0.8129, Validation Accuracy: 0.7888, Loss: 1.3368\n",
      "Epoch   2 Batch  224/269 - Train Accuracy: 0.8065, Validation Accuracy: 0.7879, Loss: 1.3518\n",
      "Epoch   2 Batch  226/269 - Train Accuracy: 0.8024, Validation Accuracy: 0.8008, Loss: 1.3618\n",
      "Epoch   2 Batch  228/269 - Train Accuracy: 0.8076, Validation Accuracy: 0.7878, Loss: 1.3535\n",
      "Epoch   2 Batch  230/269 - Train Accuracy: 0.8096, Validation Accuracy: 0.7960, Loss: 1.3337\n",
      "Epoch   2 Batch  232/269 - Train Accuracy: 0.7851, Validation Accuracy: 0.7940, Loss: 1.3675\n",
      "Epoch   2 Batch  234/269 - Train Accuracy: 0.8014, Validation Accuracy: 0.7931, Loss: 1.3271\n",
      "Epoch   2 Batch  236/269 - Train Accuracy: 0.8017, Validation Accuracy: 0.7892, Loss: 1.3353\n",
      "Epoch   2 Batch  238/269 - Train Accuracy: 0.8166, Validation Accuracy: 0.7947, Loss: 1.3033\n",
      "Epoch   2 Batch  240/269 - Train Accuracy: 0.8222, Validation Accuracy: 0.7920, Loss: 1.3189\n",
      "Epoch   2 Batch  242/269 - Train Accuracy: 0.8085, Validation Accuracy: 0.7999, Loss: 1.3140\n",
      "Epoch   2 Batch  244/269 - Train Accuracy: 0.8184, Validation Accuracy: 0.7911, Loss: 1.3527\n",
      "Epoch   2 Batch  246/269 - Train Accuracy: 0.8052, Validation Accuracy: 0.8010, Loss: 1.3664\n",
      "Epoch   2 Batch  248/269 - Train Accuracy: 0.7924, Validation Accuracy: 0.7890, Loss: 1.3256\n",
      "Epoch   2 Batch  250/269 - Train Accuracy: 0.8164, Validation Accuracy: 0.8009, Loss: 1.3418\n",
      "Epoch   2 Batch  252/269 - Train Accuracy: 0.8197, Validation Accuracy: 0.7910, Loss: 1.2944\n",
      "Epoch   2 Batch  254/269 - Train Accuracy: 0.8113, Validation Accuracy: 0.8093, Loss: 1.3036\n",
      "Epoch   2 Batch  256/269 - Train Accuracy: 0.8001, Validation Accuracy: 0.8024, Loss: 1.3558\n",
      "Epoch   2 Batch  258/269 - Train Accuracy: 0.8222, Validation Accuracy: 0.8051, Loss: 1.3107\n",
      "Epoch   2 Batch  260/269 - Train Accuracy: 0.8077, Validation Accuracy: 0.8129, Loss: 1.3501\n",
      "Epoch   2 Batch  262/269 - Train Accuracy: 0.8342, Validation Accuracy: 0.8087, Loss: 1.3148\n",
      "Epoch   2 Batch  264/269 - Train Accuracy: 0.7993, Validation Accuracy: 0.8152, Loss: 1.3339\n",
      "Epoch   2 Batch  266/269 - Train Accuracy: 0.8062, Validation Accuracy: 0.8055, Loss: 1.3144\n",
      "Epoch   3 Batch    2/269 - Train Accuracy: 0.8297, Validation Accuracy: 0.8074, Loss: 1.3277\n",
      "Epoch   3 Batch    4/269 - Train Accuracy: 0.7907, Validation Accuracy: 0.8113, Loss: 1.3386\n",
      "Epoch   3 Batch    6/269 - Train Accuracy: 0.8024, Validation Accuracy: 0.7978, Loss: 1.3295\n",
      "Epoch   3 Batch    8/269 - Train Accuracy: 0.7980, Validation Accuracy: 0.7890, Loss: 1.3377\n",
      "Epoch   3 Batch   10/269 - Train Accuracy: 0.8098, Validation Accuracy: 0.7871, Loss: 1.3380\n",
      "Epoch   3 Batch   12/269 - Train Accuracy: 0.7915, Validation Accuracy: 0.8034, Loss: 1.3273\n",
      "Epoch   3 Batch   14/269 - Train Accuracy: 0.7958, Validation Accuracy: 0.7939, Loss: 1.3256\n",
      "Epoch   3 Batch   16/269 - Train Accuracy: 0.8158, Validation Accuracy: 0.8145, Loss: 1.2705\n",
      "Epoch   3 Batch   18/269 - Train Accuracy: 0.8189, Validation Accuracy: 0.8196, Loss: 1.2919\n",
      "Epoch   3 Batch   20/269 - Train Accuracy: 0.8139, Validation Accuracy: 0.8201, Loss: 1.3005\n",
      "Epoch   3 Batch   22/269 - Train Accuracy: 0.8414, Validation Accuracy: 0.8216, Loss: 1.3026\n",
      "Epoch   3 Batch   24/269 - Train Accuracy: 0.8192, Validation Accuracy: 0.8311, Loss: 1.2711\n",
      "Epoch   3 Batch   26/269 - Train Accuracy: 0.8297, Validation Accuracy: 0.8357, Loss: 1.2661\n",
      "Epoch   3 Batch   28/269 - Train Accuracy: 0.8030, Validation Accuracy: 0.8386, Loss: 1.3447\n",
      "Epoch   3 Batch   30/269 - Train Accuracy: 0.8530, Validation Accuracy: 0.8439, Loss: 1.2633\n",
      "Epoch   3 Batch   32/269 - Train Accuracy: 0.8279, Validation Accuracy: 0.8398, Loss: 1.2973\n",
      "Epoch   3 Batch   34/269 - Train Accuracy: 0.8354, Validation Accuracy: 0.8366, Loss: 1.2755\n",
      "Epoch   3 Batch   36/269 - Train Accuracy: 0.8284, Validation Accuracy: 0.8305, Loss: 1.2814\n",
      "Epoch   3 Batch   38/269 - Train Accuracy: 0.8490, Validation Accuracy: 0.8384, Loss: 1.3021\n",
      "Epoch   3 Batch   40/269 - Train Accuracy: 0.8328, Validation Accuracy: 0.8241, Loss: 1.2915\n",
      "Epoch   3 Batch   42/269 - Train Accuracy: 0.8551, Validation Accuracy: 0.8421, Loss: 1.2730\n",
      "Epoch   3 Batch   44/269 - Train Accuracy: 0.8477, Validation Accuracy: 0.8366, Loss: 1.2258\n",
      "Epoch   3 Batch   46/269 - Train Accuracy: 0.8464, Validation Accuracy: 0.8501, Loss: 1.2621\n",
      "Epoch   3 Batch   48/269 - Train Accuracy: 0.8627, Validation Accuracy: 0.8453, Loss: 1.2662\n",
      "Epoch   3 Batch   50/269 - Train Accuracy: 0.8270, Validation Accuracy: 0.8419, Loss: 1.2869\n",
      "Epoch   3 Batch   52/269 - Train Accuracy: 0.8516, Validation Accuracy: 0.8483, Loss: 1.2216\n",
      "Epoch   3 Batch   54/269 - Train Accuracy: 0.8534, Validation Accuracy: 0.8419, Loss: 1.2172\n",
      "Epoch   3 Batch   56/269 - Train Accuracy: 0.8424, Validation Accuracy: 0.8501, Loss: 1.2650\n",
      "Epoch   3 Batch   58/269 - Train Accuracy: 0.8439, Validation Accuracy: 0.8507, Loss: 1.2233\n",
      "Epoch   3 Batch   60/269 - Train Accuracy: 0.8590, Validation Accuracy: 0.8469, Loss: 1.2274\n",
      "Epoch   3 Batch   62/269 - Train Accuracy: 0.8659, Validation Accuracy: 0.8537, Loss: 1.2746\n",
      "Epoch   3 Batch   64/269 - Train Accuracy: 0.8681, Validation Accuracy: 0.8514, Loss: 1.2494\n",
      "Epoch   3 Batch   66/269 - Train Accuracy: 0.8577, Validation Accuracy: 0.8453, Loss: 1.2715\n",
      "Epoch   3 Batch   68/269 - Train Accuracy: 0.8402, Validation Accuracy: 0.8536, Loss: 1.2509\n",
      "Epoch   3 Batch   70/269 - Train Accuracy: 0.8690, Validation Accuracy: 0.8398, Loss: 1.2611\n",
      "Epoch   3 Batch   72/269 - Train Accuracy: 0.8634, Validation Accuracy: 0.8597, Loss: 1.2278\n",
      "Epoch   3 Batch   74/269 - Train Accuracy: 0.8563, Validation Accuracy: 0.8520, Loss: 1.2447\n",
      "Epoch   3 Batch   76/269 - Train Accuracy: 0.8471, Validation Accuracy: 0.8555, Loss: 1.2455\n",
      "Epoch   3 Batch   78/269 - Train Accuracy: 0.8553, Validation Accuracy: 0.8557, Loss: 1.2330\n",
      "Epoch   3 Batch   80/269 - Train Accuracy: 0.8622, Validation Accuracy: 0.8631, Loss: 1.2405\n",
      "Epoch   3 Batch   82/269 - Train Accuracy: 0.8839, Validation Accuracy: 0.8699, Loss: 1.2145\n",
      "Epoch   3 Batch   84/269 - Train Accuracy: 0.8567, Validation Accuracy: 0.8563, Loss: 1.2260\n",
      "Epoch   3 Batch   86/269 - Train Accuracy: 0.8698, Validation Accuracy: 0.8620, Loss: 1.2166\n",
      "Epoch   3 Batch   88/269 - Train Accuracy: 0.8564, Validation Accuracy: 0.8625, Loss: 1.2321\n",
      "Epoch   3 Batch   90/269 - Train Accuracy: 0.8528, Validation Accuracy: 0.8623, Loss: 1.2490\n",
      "Epoch   3 Batch   92/269 - Train Accuracy: 0.8776, Validation Accuracy: 0.8671, Loss: 1.2308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3 Batch   94/269 - Train Accuracy: 0.8788, Validation Accuracy: 0.8669, Loss: 1.2250\n",
      "Epoch   3 Batch   96/269 - Train Accuracy: 0.8523, Validation Accuracy: 0.8705, Loss: 1.2381\n",
      "Epoch   3 Batch   98/269 - Train Accuracy: 0.8686, Validation Accuracy: 0.8803, Loss: 1.2121\n",
      "Epoch   3 Batch  100/269 - Train Accuracy: 0.8802, Validation Accuracy: 0.8704, Loss: 1.2135\n",
      "Epoch   3 Batch  102/269 - Train Accuracy: 0.8796, Validation Accuracy: 0.8733, Loss: 1.1850\n",
      "Epoch   3 Batch  104/269 - Train Accuracy: 0.8704, Validation Accuracy: 0.8657, Loss: 1.2219\n",
      "Epoch   3 Batch  106/269 - Train Accuracy: 0.8539, Validation Accuracy: 0.8686, Loss: 1.2136\n",
      "Epoch   3 Batch  108/269 - Train Accuracy: 0.8704, Validation Accuracy: 0.8635, Loss: 1.2119\n",
      "Epoch   3 Batch  110/269 - Train Accuracy: 0.8712, Validation Accuracy: 0.8759, Loss: 1.2244\n",
      "Epoch   3 Batch  112/269 - Train Accuracy: 0.8678, Validation Accuracy: 0.8627, Loss: 1.2039\n",
      "Epoch   3 Batch  114/269 - Train Accuracy: 0.8736, Validation Accuracy: 0.8788, Loss: 1.2028\n",
      "Epoch   3 Batch  116/269 - Train Accuracy: 0.8899, Validation Accuracy: 0.8875, Loss: 1.2001\n",
      "Epoch   3 Batch  118/269 - Train Accuracy: 0.8843, Validation Accuracy: 0.8788, Loss: 1.1964\n",
      "Epoch   3 Batch  120/269 - Train Accuracy: 0.8815, Validation Accuracy: 0.8798, Loss: 1.2073\n",
      "Epoch   3 Batch  122/269 - Train Accuracy: 0.8769, Validation Accuracy: 0.8731, Loss: 1.2141\n",
      "Epoch   3 Batch  124/269 - Train Accuracy: 0.8949, Validation Accuracy: 0.8690, Loss: 1.2214\n",
      "Epoch   3 Batch  126/269 - Train Accuracy: 0.8694, Validation Accuracy: 0.8813, Loss: 1.2056\n",
      "Epoch   3 Batch  128/269 - Train Accuracy: 0.8909, Validation Accuracy: 0.8770, Loss: 1.1779\n",
      "Epoch   3 Batch  130/269 - Train Accuracy: 0.8861, Validation Accuracy: 0.8822, Loss: 1.2081\n",
      "Epoch   3 Batch  132/269 - Train Accuracy: 0.8857, Validation Accuracy: 0.8882, Loss: 1.1852\n",
      "Epoch   3 Batch  134/269 - Train Accuracy: 0.8681, Validation Accuracy: 0.8952, Loss: 1.2013\n",
      "Epoch   3 Batch  136/269 - Train Accuracy: 0.8703, Validation Accuracy: 0.8737, Loss: 1.1959\n",
      "Epoch   3 Batch  138/269 - Train Accuracy: 0.8697, Validation Accuracy: 0.8674, Loss: 1.2004\n",
      "Epoch   3 Batch  140/269 - Train Accuracy: 0.8891, Validation Accuracy: 0.8678, Loss: 1.2137\n",
      "Epoch   3 Batch  142/269 - Train Accuracy: 0.8945, Validation Accuracy: 0.8728, Loss: 1.1795\n",
      "Epoch   3 Batch  144/269 - Train Accuracy: 0.8833, Validation Accuracy: 0.8736, Loss: 1.1854\n",
      "Epoch   3 Batch  146/269 - Train Accuracy: 0.8877, Validation Accuracy: 0.8892, Loss: 1.1910\n",
      "Epoch   3 Batch  148/269 - Train Accuracy: 0.8730, Validation Accuracy: 0.8820, Loss: 1.1845\n",
      "Epoch   3 Batch  150/269 - Train Accuracy: 0.8790, Validation Accuracy: 0.8917, Loss: 1.1730\n",
      "Epoch   3 Batch  152/269 - Train Accuracy: 0.8853, Validation Accuracy: 0.8903, Loss: 1.1487\n",
      "Epoch   3 Batch  154/269 - Train Accuracy: 0.9094, Validation Accuracy: 0.8857, Loss: 1.2046\n",
      "Epoch   3 Batch  156/269 - Train Accuracy: 0.8823, Validation Accuracy: 0.8903, Loss: 1.1886\n",
      "Epoch   3 Batch  158/269 - Train Accuracy: 0.8832, Validation Accuracy: 0.8954, Loss: 1.1808\n",
      "Epoch   3 Batch  160/269 - Train Accuracy: 0.8916, Validation Accuracy: 0.8908, Loss: 1.1921\n",
      "Epoch   3 Batch  162/269 - Train Accuracy: 0.8913, Validation Accuracy: 0.8896, Loss: 1.1835\n",
      "Epoch   3 Batch  164/269 - Train Accuracy: 0.8941, Validation Accuracy: 0.8959, Loss: 1.1844\n",
      "Epoch   3 Batch  166/269 - Train Accuracy: 0.8869, Validation Accuracy: 0.8890, Loss: 1.1640\n",
      "Epoch   3 Batch  168/269 - Train Accuracy: 0.8924, Validation Accuracy: 0.8904, Loss: 1.2057\n",
      "Epoch   3 Batch  170/269 - Train Accuracy: 0.9009, Validation Accuracy: 0.9011, Loss: 1.1789\n",
      "Epoch   3 Batch  172/269 - Train Accuracy: 0.8879, Validation Accuracy: 0.8885, Loss: 1.2077\n",
      "Epoch   3 Batch  174/269 - Train Accuracy: 0.8881, Validation Accuracy: 0.8873, Loss: 1.1600\n",
      "Epoch   3 Batch  176/269 - Train Accuracy: 0.8841, Validation Accuracy: 0.8907, Loss: 1.1509\n",
      "Epoch   3 Batch  178/269 - Train Accuracy: 0.8970, Validation Accuracy: 0.9030, Loss: 1.1822\n",
      "Epoch   3 Batch  180/269 - Train Accuracy: 0.9138, Validation Accuracy: 0.8990, Loss: 1.1676\n",
      "Epoch   3 Batch  182/269 - Train Accuracy: 0.9013, Validation Accuracy: 0.9008, Loss: 1.1738\n",
      "Epoch   3 Batch  184/269 - Train Accuracy: 0.8967, Validation Accuracy: 0.9086, Loss: 1.1724\n",
      "Epoch   3 Batch  186/269 - Train Accuracy: 0.8870, Validation Accuracy: 0.9087, Loss: 1.1474\n",
      "Epoch   3 Batch  188/269 - Train Accuracy: 0.9141, Validation Accuracy: 0.8999, Loss: 1.1697\n",
      "Epoch   3 Batch  190/269 - Train Accuracy: 0.8959, Validation Accuracy: 0.8918, Loss: 1.1199\n",
      "Epoch   3 Batch  192/269 - Train Accuracy: 0.8983, Validation Accuracy: 0.8911, Loss: 1.1831\n",
      "Epoch   3 Batch  194/269 - Train Accuracy: 0.9022, Validation Accuracy: 0.8957, Loss: 1.1701\n",
      "Epoch   3 Batch  196/269 - Train Accuracy: 0.9049, Validation Accuracy: 0.8949, Loss: 1.1611\n",
      "Epoch   3 Batch  198/269 - Train Accuracy: 0.8778, Validation Accuracy: 0.8968, Loss: 1.1380\n",
      "Epoch   3 Batch  200/269 - Train Accuracy: 0.8986, Validation Accuracy: 0.9038, Loss: 1.1428\n",
      "Epoch   3 Batch  202/269 - Train Accuracy: 0.9024, Validation Accuracy: 0.8949, Loss: 1.1587\n",
      "Epoch   3 Batch  204/269 - Train Accuracy: 0.8926, Validation Accuracy: 0.9063, Loss: 1.1513\n",
      "Epoch   3 Batch  206/269 - Train Accuracy: 0.9007, Validation Accuracy: 0.8997, Loss: 1.1773\n",
      "Epoch   3 Batch  208/269 - Train Accuracy: 0.9152, Validation Accuracy: 0.9089, Loss: 1.1640\n",
      "Epoch   3 Batch  210/269 - Train Accuracy: 0.8968, Validation Accuracy: 0.9101, Loss: 1.1383\n",
      "Epoch   3 Batch  212/269 - Train Accuracy: 0.8813, Validation Accuracy: 0.8921, Loss: 1.1310\n",
      "Epoch   3 Batch  214/269 - Train Accuracy: 0.9016, Validation Accuracy: 0.9078, Loss: 1.1660\n",
      "Epoch   3 Batch  216/269 - Train Accuracy: 0.8947, Validation Accuracy: 0.9101, Loss: 1.1922\n",
      "Epoch   3 Batch  218/269 - Train Accuracy: 0.9074, Validation Accuracy: 0.9140, Loss: 1.1598\n",
      "Epoch   3 Batch  220/269 - Train Accuracy: 0.9015, Validation Accuracy: 0.9150, Loss: 1.1326\n",
      "Epoch   3 Batch  222/269 - Train Accuracy: 0.9156, Validation Accuracy: 0.9189, Loss: 1.1025\n",
      "Epoch   3 Batch  224/269 - Train Accuracy: 0.9065, Validation Accuracy: 0.9061, Loss: 1.1822\n",
      "Epoch   3 Batch  226/269 - Train Accuracy: 0.9104, Validation Accuracy: 0.9094, Loss: 1.1599\n",
      "Epoch   3 Batch  228/269 - Train Accuracy: 0.9019, Validation Accuracy: 0.9072, Loss: 1.1507\n",
      "Epoch   3 Batch  230/269 - Train Accuracy: 0.9122, Validation Accuracy: 0.9070, Loss: 1.1550\n",
      "Epoch   3 Batch  232/269 - Train Accuracy: 0.8925, Validation Accuracy: 0.9131, Loss: 1.1424\n",
      "Epoch   3 Batch  234/269 - Train Accuracy: 0.9043, Validation Accuracy: 0.9162, Loss: 1.1341\n",
      "Epoch   3 Batch  236/269 - Train Accuracy: 0.9096, Validation Accuracy: 0.9140, Loss: 1.1454\n",
      "Epoch   3 Batch  238/269 - Train Accuracy: 0.9129, Validation Accuracy: 0.9096, Loss: 1.1394\n",
      "Epoch   3 Batch  240/269 - Train Accuracy: 0.9192, Validation Accuracy: 0.9099, Loss: 1.1329\n",
      "Epoch   3 Batch  242/269 - Train Accuracy: 0.9218, Validation Accuracy: 0.9074, Loss: 1.1431\n",
      "Epoch   3 Batch  244/269 - Train Accuracy: 0.9082, Validation Accuracy: 0.9126, Loss: 1.1309\n",
      "Epoch   3 Batch  246/269 - Train Accuracy: 0.9071, Validation Accuracy: 0.9197, Loss: 1.1363\n",
      "Epoch   3 Batch  248/269 - Train Accuracy: 0.9077, Validation Accuracy: 0.9242, Loss: 1.1544\n",
      "Epoch   3 Batch  250/269 - Train Accuracy: 0.9315, Validation Accuracy: 0.9154, Loss: 1.1169\n",
      "Epoch   3 Batch  252/269 - Train Accuracy: 0.9218, Validation Accuracy: 0.9108, Loss: 1.1242\n",
      "Epoch   3 Batch  254/269 - Train Accuracy: 0.9182, Validation Accuracy: 0.9226, Loss: 1.1178\n",
      "Epoch   3 Batch  256/269 - Train Accuracy: 0.8933, Validation Accuracy: 0.9094, Loss: 1.1074\n",
      "Epoch   3 Batch  258/269 - Train Accuracy: 0.9151, Validation Accuracy: 0.9115, Loss: 1.1379\n",
      "Epoch   3 Batch  260/269 - Train Accuracy: 0.9091, Validation Accuracy: 0.9168, Loss: 1.1424\n",
      "Epoch   3 Batch  262/269 - Train Accuracy: 0.9131, Validation Accuracy: 0.9180, Loss: 1.1181\n",
      "Epoch   3 Batch  264/269 - Train Accuracy: 0.8990, Validation Accuracy: 0.9169, Loss: 1.1474\n",
      "Epoch   3 Batch  266/269 - Train Accuracy: 0.9126, Validation Accuracy: 0.9093, Loss: 1.1118\n",
      "Epoch   4 Batch    2/269 - Train Accuracy: 0.9121, Validation Accuracy: 0.9145, Loss: 1.1007\n",
      "Epoch   4 Batch    4/269 - Train Accuracy: 0.9068, Validation Accuracy: 0.9141, Loss: 1.1312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4 Batch    6/269 - Train Accuracy: 0.9313, Validation Accuracy: 0.9157, Loss: 1.1103\n",
      "Epoch   4 Batch    8/269 - Train Accuracy: 0.9219, Validation Accuracy: 0.9158, Loss: 1.1656\n",
      "Epoch   4 Batch   10/269 - Train Accuracy: 0.9176, Validation Accuracy: 0.9215, Loss: 1.1239\n",
      "Epoch   4 Batch   12/269 - Train Accuracy: 0.9148, Validation Accuracy: 0.9190, Loss: 1.1575\n",
      "Epoch   4 Batch   14/269 - Train Accuracy: 0.9083, Validation Accuracy: 0.9229, Loss: 1.1081\n",
      "Epoch   4 Batch   16/269 - Train Accuracy: 0.9142, Validation Accuracy: 0.9181, Loss: 1.0929\n",
      "Epoch   4 Batch   18/269 - Train Accuracy: 0.9201, Validation Accuracy: 0.9175, Loss: 1.1290\n",
      "Epoch   4 Batch   20/269 - Train Accuracy: 0.9212, Validation Accuracy: 0.9150, Loss: 1.0994\n",
      "Epoch   4 Batch   22/269 - Train Accuracy: 0.9208, Validation Accuracy: 0.9238, Loss: 1.1349\n",
      "Epoch   4 Batch   24/269 - Train Accuracy: 0.9149, Validation Accuracy: 0.9190, Loss: 1.1363\n",
      "Epoch   4 Batch   26/269 - Train Accuracy: 0.9275, Validation Accuracy: 0.9128, Loss: 1.0958\n",
      "Epoch   4 Batch   28/269 - Train Accuracy: 0.8949, Validation Accuracy: 0.9102, Loss: 1.1305\n",
      "Epoch   4 Batch   30/269 - Train Accuracy: 0.9233, Validation Accuracy: 0.9173, Loss: 1.1305\n",
      "Epoch   4 Batch   32/269 - Train Accuracy: 0.9238, Validation Accuracy: 0.9140, Loss: 1.1341\n",
      "Epoch   4 Batch   34/269 - Train Accuracy: 0.9161, Validation Accuracy: 0.9150, Loss: 1.1285\n",
      "Epoch   4 Batch   36/269 - Train Accuracy: 0.9070, Validation Accuracy: 0.9213, Loss: 1.1477\n",
      "Epoch   4 Batch   38/269 - Train Accuracy: 0.9067, Validation Accuracy: 0.9219, Loss: 1.1395\n",
      "Epoch   4 Batch   40/269 - Train Accuracy: 0.9080, Validation Accuracy: 0.9221, Loss: 1.1436\n",
      "Epoch   4 Batch   42/269 - Train Accuracy: 0.9234, Validation Accuracy: 0.9272, Loss: 1.1008\n",
      "Epoch   4 Batch   44/269 - Train Accuracy: 0.9170, Validation Accuracy: 0.9202, Loss: 1.1205\n",
      "Epoch   4 Batch   46/269 - Train Accuracy: 0.9080, Validation Accuracy: 0.9184, Loss: 1.1075\n",
      "Epoch   4 Batch   48/269 - Train Accuracy: 0.9261, Validation Accuracy: 0.9238, Loss: 1.1098\n",
      "Epoch   4 Batch   50/269 - Train Accuracy: 0.8971, Validation Accuracy: 0.9231, Loss: 1.1594\n",
      "Epoch   4 Batch   52/269 - Train Accuracy: 0.9214, Validation Accuracy: 0.9244, Loss: 1.1299\n",
      "Epoch   4 Batch   54/269 - Train Accuracy: 0.9228, Validation Accuracy: 0.9209, Loss: 1.1416\n",
      "Epoch   4 Batch   56/269 - Train Accuracy: 0.9158, Validation Accuracy: 0.9262, Loss: 1.1053\n",
      "Epoch   4 Batch   58/269 - Train Accuracy: 0.9103, Validation Accuracy: 0.9232, Loss: 1.1351\n",
      "Epoch   4 Batch   60/269 - Train Accuracy: 0.9127, Validation Accuracy: 0.9235, Loss: 1.0946\n",
      "Epoch   4 Batch   62/269 - Train Accuracy: 0.9179, Validation Accuracy: 0.9237, Loss: 1.1253\n",
      "Epoch   4 Batch   64/269 - Train Accuracy: 0.9199, Validation Accuracy: 0.9228, Loss: 1.0862\n",
      "Epoch   4 Batch   66/269 - Train Accuracy: 0.9185, Validation Accuracy: 0.9239, Loss: 1.1172\n",
      "Epoch   4 Batch   68/269 - Train Accuracy: 0.9183, Validation Accuracy: 0.9244, Loss: 1.1015\n",
      "Epoch   4 Batch   70/269 - Train Accuracy: 0.9222, Validation Accuracy: 0.9238, Loss: 1.1108\n",
      "Epoch   4 Batch   72/269 - Train Accuracy: 0.9181, Validation Accuracy: 0.9310, Loss: 1.1265\n",
      "Epoch   4 Batch   74/269 - Train Accuracy: 0.9333, Validation Accuracy: 0.9294, Loss: 1.1304\n",
      "Epoch   4 Batch   76/269 - Train Accuracy: 0.9097, Validation Accuracy: 0.9273, Loss: 1.1063\n",
      "Epoch   4 Batch   78/269 - Train Accuracy: 0.9227, Validation Accuracy: 0.9296, Loss: 1.0918\n",
      "Epoch   4 Batch   80/269 - Train Accuracy: 0.9345, Validation Accuracy: 0.9241, Loss: 1.1058\n",
      "Epoch   4 Batch   82/269 - Train Accuracy: 0.9347, Validation Accuracy: 0.9260, Loss: 1.0919\n",
      "Epoch   4 Batch   84/269 - Train Accuracy: 0.9194, Validation Accuracy: 0.9150, Loss: 1.1178\n",
      "Epoch   4 Batch   86/269 - Train Accuracy: 0.9181, Validation Accuracy: 0.9221, Loss: 1.0904\n",
      "Epoch   4 Batch   88/269 - Train Accuracy: 0.9182, Validation Accuracy: 0.9300, Loss: 1.1099\n",
      "Epoch   4 Batch   90/269 - Train Accuracy: 0.9316, Validation Accuracy: 0.9291, Loss: 1.0941\n",
      "Epoch   4 Batch   92/269 - Train Accuracy: 0.9365, Validation Accuracy: 0.9324, Loss: 1.0720\n",
      "Epoch   4 Batch   94/269 - Train Accuracy: 0.9290, Validation Accuracy: 0.9281, Loss: 1.1340\n",
      "Epoch   4 Batch   96/269 - Train Accuracy: 0.9167, Validation Accuracy: 0.9241, Loss: 1.1338\n",
      "Epoch   4 Batch   98/269 - Train Accuracy: 0.9200, Validation Accuracy: 0.9290, Loss: 1.0902\n",
      "Epoch   4 Batch  100/269 - Train Accuracy: 0.9235, Validation Accuracy: 0.9303, Loss: 1.0842\n",
      "Epoch   4 Batch  102/269 - Train Accuracy: 0.9236, Validation Accuracy: 0.9339, Loss: 1.0887\n",
      "Epoch   4 Batch  104/269 - Train Accuracy: 0.9256, Validation Accuracy: 0.9230, Loss: 1.0838\n",
      "Epoch   4 Batch  106/269 - Train Accuracy: 0.9194, Validation Accuracy: 0.9241, Loss: 1.0610\n",
      "Epoch   4 Batch  108/269 - Train Accuracy: 0.9375, Validation Accuracy: 0.9279, Loss: 1.1399\n",
      "Epoch   4 Batch  110/269 - Train Accuracy: 0.9225, Validation Accuracy: 0.9248, Loss: 1.0853\n",
      "Epoch   4 Batch  112/269 - Train Accuracy: 0.9249, Validation Accuracy: 0.9275, Loss: 1.0703\n",
      "Epoch   4 Batch  114/269 - Train Accuracy: 0.9245, Validation Accuracy: 0.9276, Loss: 1.1490\n",
      "Epoch   4 Batch  116/269 - Train Accuracy: 0.9376, Validation Accuracy: 0.9250, Loss: 1.1133\n",
      "Epoch   4 Batch  118/269 - Train Accuracy: 0.9331, Validation Accuracy: 0.9237, Loss: 1.0815\n",
      "Epoch   4 Batch  120/269 - Train Accuracy: 0.9293, Validation Accuracy: 0.9270, Loss: 1.0902\n",
      "Epoch   4 Batch  122/269 - Train Accuracy: 0.9206, Validation Accuracy: 0.9231, Loss: 1.1008\n",
      "Epoch   4 Batch  124/269 - Train Accuracy: 0.9309, Validation Accuracy: 0.9282, Loss: 1.1114\n",
      "Epoch   4 Batch  126/269 - Train Accuracy: 0.9209, Validation Accuracy: 0.9257, Loss: 1.1285\n",
      "Epoch   4 Batch  128/269 - Train Accuracy: 0.9284, Validation Accuracy: 0.9165, Loss: 1.1290\n",
      "Epoch   4 Batch  130/269 - Train Accuracy: 0.9419, Validation Accuracy: 0.9238, Loss: 1.1055\n",
      "Epoch   4 Batch  132/269 - Train Accuracy: 0.9242, Validation Accuracy: 0.9317, Loss: 1.0859\n",
      "Epoch   4 Batch  134/269 - Train Accuracy: 0.9240, Validation Accuracy: 0.9370, Loss: 1.1092\n",
      "Epoch   4 Batch  136/269 - Train Accuracy: 0.9155, Validation Accuracy: 0.9346, Loss: 1.0935\n",
      "Epoch   4 Batch  138/269 - Train Accuracy: 0.9290, Validation Accuracy: 0.9285, Loss: 1.1083\n",
      "Epoch   4 Batch  140/269 - Train Accuracy: 0.9290, Validation Accuracy: 0.9329, Loss: 1.1296\n",
      "Epoch   4 Batch  142/269 - Train Accuracy: 0.9368, Validation Accuracy: 0.9346, Loss: 1.0820\n",
      "Epoch   4 Batch  144/269 - Train Accuracy: 0.9416, Validation Accuracy: 0.9327, Loss: 1.1032\n",
      "Epoch   4 Batch  146/269 - Train Accuracy: 0.9313, Validation Accuracy: 0.9310, Loss: 1.0857\n",
      "Epoch   4 Batch  148/269 - Train Accuracy: 0.9302, Validation Accuracy: 0.9323, Loss: 1.1055\n",
      "Epoch   4 Batch  150/269 - Train Accuracy: 0.9232, Validation Accuracy: 0.9354, Loss: 1.1358\n",
      "Epoch   4 Batch  152/269 - Train Accuracy: 0.9275, Validation Accuracy: 0.9285, Loss: 1.0914\n",
      "Epoch   4 Batch  154/269 - Train Accuracy: 0.9446, Validation Accuracy: 0.9361, Loss: 1.1006\n",
      "Epoch   4 Batch  156/269 - Train Accuracy: 0.9419, Validation Accuracy: 0.9424, Loss: 1.1196\n",
      "Epoch   4 Batch  158/269 - Train Accuracy: 0.9307, Validation Accuracy: 0.9361, Loss: 1.0916\n",
      "Epoch   4 Batch  160/269 - Train Accuracy: 0.9188, Validation Accuracy: 0.9347, Loss: 1.0965\n",
      "Epoch   4 Batch  162/269 - Train Accuracy: 0.9321, Validation Accuracy: 0.9335, Loss: 1.1000\n",
      "Epoch   4 Batch  164/269 - Train Accuracy: 0.9408, Validation Accuracy: 0.9441, Loss: 1.0964\n",
      "Epoch   4 Batch  166/269 - Train Accuracy: 0.9319, Validation Accuracy: 0.9396, Loss: 1.0657\n",
      "Epoch   4 Batch  168/269 - Train Accuracy: 0.9399, Validation Accuracy: 0.9447, Loss: 1.0835\n",
      "Epoch   4 Batch  170/269 - Train Accuracy: 0.9289, Validation Accuracy: 0.9431, Loss: 1.0680\n",
      "Epoch   4 Batch  172/269 - Train Accuracy: 0.9261, Validation Accuracy: 0.9400, Loss: 1.0612\n",
      "Epoch   4 Batch  174/269 - Train Accuracy: 0.9350, Validation Accuracy: 0.9355, Loss: 1.1166\n",
      "Epoch   4 Batch  176/269 - Train Accuracy: 0.9280, Validation Accuracy: 0.9474, Loss: 1.1177\n",
      "Epoch   4 Batch  178/269 - Train Accuracy: 0.9403, Validation Accuracy: 0.9402, Loss: 1.1142\n",
      "Epoch   4 Batch  180/269 - Train Accuracy: 0.9427, Validation Accuracy: 0.9434, Loss: 1.0673\n",
      "Epoch   4 Batch  182/269 - Train Accuracy: 0.9385, Validation Accuracy: 0.9424, Loss: 1.0727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4 Batch  184/269 - Train Accuracy: 0.9385, Validation Accuracy: 0.9418, Loss: 1.0936\n",
      "Epoch   4 Batch  186/269 - Train Accuracy: 0.9309, Validation Accuracy: 0.9367, Loss: 1.1010\n",
      "Epoch   4 Batch  188/269 - Train Accuracy: 0.9360, Validation Accuracy: 0.9407, Loss: 1.0798\n",
      "Epoch   4 Batch  190/269 - Train Accuracy: 0.9374, Validation Accuracy: 0.9425, Loss: 1.1005\n",
      "Epoch   4 Batch  192/269 - Train Accuracy: 0.9430, Validation Accuracy: 0.9317, Loss: 1.0942\n",
      "Epoch   4 Batch  194/269 - Train Accuracy: 0.9385, Validation Accuracy: 0.9388, Loss: 1.1215\n",
      "Epoch   4 Batch  196/269 - Train Accuracy: 0.9434, Validation Accuracy: 0.9387, Loss: 1.0788\n",
      "Epoch   4 Batch  198/269 - Train Accuracy: 0.9193, Validation Accuracy: 0.9359, Loss: 1.1133\n",
      "Epoch   4 Batch  200/269 - Train Accuracy: 0.9399, Validation Accuracy: 0.9344, Loss: 1.1310\n",
      "Epoch   4 Batch  202/269 - Train Accuracy: 0.9343, Validation Accuracy: 0.9303, Loss: 1.0656\n",
      "Epoch   4 Batch  204/269 - Train Accuracy: 0.9212, Validation Accuracy: 0.9345, Loss: 1.0826\n",
      "Epoch   4 Batch  206/269 - Train Accuracy: 0.9310, Validation Accuracy: 0.9414, Loss: 1.0866\n",
      "Epoch   4 Batch  208/269 - Train Accuracy: 0.9399, Validation Accuracy: 0.9461, Loss: 1.0699\n",
      "Epoch   4 Batch  210/269 - Train Accuracy: 0.9401, Validation Accuracy: 0.9487, Loss: 1.1046\n",
      "Epoch   4 Batch  212/269 - Train Accuracy: 0.9268, Validation Accuracy: 0.9467, Loss: 1.1435\n",
      "Epoch   4 Batch  214/269 - Train Accuracy: 0.9439, Validation Accuracy: 0.9436, Loss: 1.1000\n",
      "Epoch   4 Batch  216/269 - Train Accuracy: 0.9325, Validation Accuracy: 0.9407, Loss: 1.1011\n",
      "Epoch   4 Batch  218/269 - Train Accuracy: 0.9367, Validation Accuracy: 0.9394, Loss: 1.0736\n",
      "Epoch   4 Batch  220/269 - Train Accuracy: 0.9395, Validation Accuracy: 0.9410, Loss: 1.0866\n",
      "Epoch   4 Batch  222/269 - Train Accuracy: 0.9533, Validation Accuracy: 0.9442, Loss: 1.0760\n",
      "Epoch   4 Batch  224/269 - Train Accuracy: 0.9398, Validation Accuracy: 0.9416, Loss: 1.1101\n",
      "Epoch   4 Batch  226/269 - Train Accuracy: 0.9302, Validation Accuracy: 0.9443, Loss: 1.1154\n",
      "Epoch   4 Batch  228/269 - Train Accuracy: 0.9426, Validation Accuracy: 0.9494, Loss: 1.1091\n",
      "Epoch   4 Batch  230/269 - Train Accuracy: 0.9399, Validation Accuracy: 0.9524, Loss: 1.0834\n",
      "Epoch   4 Batch  232/269 - Train Accuracy: 0.9335, Validation Accuracy: 0.9482, Loss: 1.0545\n",
      "Epoch   4 Batch  234/269 - Train Accuracy: 0.9315, Validation Accuracy: 0.9409, Loss: 1.0732\n",
      "Epoch   4 Batch  236/269 - Train Accuracy: 0.9384, Validation Accuracy: 0.9436, Loss: 1.1039\n",
      "Epoch   4 Batch  238/269 - Train Accuracy: 0.9386, Validation Accuracy: 0.9492, Loss: 1.0970\n",
      "Epoch   4 Batch  240/269 - Train Accuracy: 0.9499, Validation Accuracy: 0.9435, Loss: 1.0806\n",
      "Epoch   4 Batch  242/269 - Train Accuracy: 0.9465, Validation Accuracy: 0.9454, Loss: 1.1067\n",
      "Epoch   4 Batch  244/269 - Train Accuracy: 0.9376, Validation Accuracy: 0.9422, Loss: 1.0817\n",
      "Epoch   4 Batch  246/269 - Train Accuracy: 0.9356, Validation Accuracy: 0.9356, Loss: 1.0862\n",
      "Epoch   4 Batch  248/269 - Train Accuracy: 0.9482, Validation Accuracy: 0.9395, Loss: 1.0913\n",
      "Epoch   4 Batch  250/269 - Train Accuracy: 0.9536, Validation Accuracy: 0.9406, Loss: 1.0741\n",
      "Epoch   4 Batch  252/269 - Train Accuracy: 0.9544, Validation Accuracy: 0.9402, Loss: 1.0551\n",
      "Epoch   4 Batch  254/269 - Train Accuracy: 0.9450, Validation Accuracy: 0.9402, Loss: 1.0720\n",
      "Epoch   4 Batch  256/269 - Train Accuracy: 0.9239, Validation Accuracy: 0.9431, Loss: 1.0837\n",
      "Epoch   4 Batch  258/269 - Train Accuracy: 0.9383, Validation Accuracy: 0.9377, Loss: 1.0950\n",
      "Epoch   4 Batch  260/269 - Train Accuracy: 0.9494, Validation Accuracy: 0.9475, Loss: 1.1072\n",
      "Epoch   4 Batch  262/269 - Train Accuracy: 0.9388, Validation Accuracy: 0.9512, Loss: 1.1185\n",
      "Epoch   4 Batch  264/269 - Train Accuracy: 0.6576, Validation Accuracy: 0.6805, Loss: 1.1303\n",
      "Epoch   4 Batch  266/269 - Train Accuracy: 0.3978, Validation Accuracy: 0.4276, Loss: 2.5758\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "\n",
    "\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     source_sequence_length: sources_lengths,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     source_sequence_length: valid_sources_lengths,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Parameters\n",
    "Save the `batch_size` and `save_path` parameters for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "helper.save_params(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper.load_preprocess()\n",
    "load_path = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence\n",
    "To feed a sentence into the model for translation, you first need to preprocess it.  Implement the function `sentence_to_seq()` to preprocess new sentences.\n",
    "\n",
    "- Convert the sentence to lowercase\n",
    "- Convert words into ids using `vocab_to_int`\n",
    " - Convert words not in the vocabulary, to the `<UNK>` word id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a sequence of ids\n",
    "    :param sentence: String\n",
    "    :param vocab_to_int: Dictionary to go from the words to an id\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = sentence.lower()\n",
    "    return [vocab_to_int.get(unk, vocab_to_int['<UNK>']) for unk in sentence.split()]\n",
    "\n",
    "tests.test_sentence_to_seq(sentence_to_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate\n",
    "This will translate `translate_sentence` from English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [60, 82, 166, 162, 38, 94, 211]\n",
      "  English Words: ['he', 'saw', 'a', 'old', 'yellow', 'truck', '.']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [156, 285, 190, 275, 275, 201, 201, 175, 284, 275, 259, 76, 1]\n",
      "  French Words: il est le l' l' de de d' automne l' citrons . <EOS>\n"
     ]
    }
   ],
   "source": [
    "translate_sentence = 'he saw a old yellow truck .'\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         source_sequence_length: [len(translate_sentence)]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imperfect Translation\n",
    "You might notice that some sentences translate better than others.  Since the dataset you're using only has a vocabulary of 227 English words of the thousands that you use, you're only going to see good results using these words.  For this project, you don't need a perfect translation. However, if you want to create a better translation model, you'll need better data.\n",
    "\n",
    "You can train on the [WMT10 French-English corpus](http://www.statmt.org/wmt10/training-giga-fren.tar).  This dataset has more vocabulary and richer in topics discussed.  However, this will take you days to train, so make sure you've a GPU and the neural network is performing well on dataset we provided.  Just make sure you play with the WMT10 corpus after you've submitted this project.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_language_translation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
